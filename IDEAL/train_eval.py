import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score, average_precision_score
#from gcn_model import GCN
import numpy as np
import time
import copy # <--- åœ¨è¿™é‡Œæ·»åŠ 
from gcn_model import GradientReverse, Discriminator
from torch.cuda.amp import autocast, GradScaler

from gcn_model import  MLPDecoder

# train_eval.py
from gcn_model import GCNWithMLP, MLPDecoder
import numpy as np
import os                # <<-- æ·»åŠ è¿™ä¸€è¡Œ
import matplotlib.pyplot as plt

# åœ¨å¾®è°ƒé˜¶æ®µï¼Œå¯¹é½MLPå°†è®­ç»ƒ
# def train_alignment_mlps(
#     drugvirus_feats, mdad_feats, device, epochs=100, lr=0.001
# ):
#     mlp_list = []
#     optim_params = []
#     for dv_feat, mdad_feat in zip(drugvirus_feats, mdad_feats):
#         mlp = torch.nn.Sequential(
#             torch.nn.Linear(dv_feat.shape[1], mdad_feat.shape[1]),
#             torch.nn.ReLU()
#         ).to(device)
#         mlp_list.append(mlp)
#         optim_params += list(mlp.parameters())
#     optimizer = torch.optim.Adam(optim_params, lr=lr,weight_decay=5e-4)
#     target_means = [torch.tensor(m.mean(axis=0), dtype=torch.float32, device=device) for m in mdad_feats]
#     target_stds = [torch.tensor(m.std(axis=0), dtype=torch.float32, device=device) for m in mdad_feats]
#     dv_tensors = [torch.tensor(f, dtype=torch.float32, device=device) for f in drugvirus_feats]
#     for epoch in range(epochs):
#         optimizer.zero_grad()
#         losses = []
#         for i, mlp in enumerate(mlp_list):
#             pred = mlp(dv_tensors[i])
#             loss = ((pred.mean(dim=0) - target_means[i]) ** 2).mean() + \
#                    ((pred.std(dim=0) - target_stds[i]) ** 2).mean()
#             losses.append(loss)
#         total_loss = sum(losses)
#         total_loss.backward()
#         optimizer.step()
#         if (epoch + 1) % 10 == 0:
#             print(f"Align Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}")
#     return mlp_list
#


# def train_alignment_mlps(
#         drugvirus_feats,
#         mdad_feats,
#         device,
#         epochs: int = 200,
#         lr: float = 1e-3,
#         batch_size: int = 256,
# ):
#
#     """
#     è®­ç»ƒä¸€ç»„ alignment MLPï¼Œå°† DrugVirus ç‰¹å¾æ˜ å°„åˆ° MDAD ç‰¹å¾ç©ºé—´ã€‚
#     loss = å‡å€¼å·® + æ–¹å·®å·® + CORAL(åæ–¹å·®å·®) + éšæœºé…å¯¹ L2ã€‚
#     """
#     from torch.utils.data import TensorDataset, DataLoader
#
#     def _make_mlp(in_dim, out_dim):
#         mlp = torch.nn.Sequential(
#             torch.nn.Linear(in_dim, 2 * out_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(2 * out_dim, out_dim),
#         ).to(device)
#         torch.nn.init.eye_(mlp[-1].weight)
#         torch.nn.init.zeros_(mlp[-1].bias)
#         return mlp
#
#
#
#     mlp_list, optim_params, dv_loaders = [], [], []
#
#     for dv_feat, mdad_feat in zip(drugvirus_feats, mdad_feats):
#         dv_tensor = torch.tensor(dv_feat, dtype=torch.float32, device=device)
#         mlp = _make_mlp(dv_tensor.size(1), mdad_feat.shape[1])
#         mlp_list.append(mlp)
#         optim_params += list(mlp.parameters())
#
#         # å¦‚æœæ ·æœ¬æ•°å°äºbatch_sizeï¼Œdrop_last=True ä¼šå¯¼è‡´loaderä¸ºç©º
#         # æ”¹ä¸º drop_last=False ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªbatch
#         drop_last_flag = dv_tensor.size(0) > batch_size
#         #dv_ds = TensorDataset(dv_tensor)
#         mdad_tensor = torch.tensor(mdad_feat, dtype=torch.float32, device=device)
#         # ç¡®ä¿è¾“å…¥å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°é‡ä¸€è‡´
#         assert dv_tensor.size(0) == mdad_tensor.size(0), "è¾“å…¥å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°å¿…é¡»ä¸€è‡´"
#
#         # å°†è¾“å…¥(X)å’Œæ ‡ç­¾(Y)æ‰“åŒ…æˆå¯¹
#         dv_ds = TensorDataset(dv_tensor, mdad_tensor)
#
#         dv_loaders.append(DataLoader(dv_ds, batch_size=batch_size, shuffle=True, drop_last=drop_last_flag))
#
#     optimizer = torch.optim.Adam(optim_params, lr=lr, weight_decay=5e-4)
#     # ä½¿ç”¨æ ‡å‡†çš„å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
#     criterion = torch.nn.MSELoss()
#
#     for epoch in range(epochs):
#         total_epoch_loss = 0.0
#         batch_count = 0
#
#         for loader_idx, dv_loader in enumerate(dv_loaders):
#             # éå†æ•°æ®åŠ è½½å™¨ï¼Œæ¯æ¬¡å¾—åˆ°ä¸€ä¸ªæ‰¹æ¬¡çš„è¾“å…¥(x)å’Œæ ‡ç­¾(y)
#             for x_batch, y_batch in dv_loader:
#                 optimizer.zero_grad()
#
#                 mlp = mlp_list[loader_idx]
#
#                 # æ¨¡å‹è¿›è¡Œé¢„æµ‹
#                 pred = mlp(x_batch)
#
#                 # ç›´æ¥è®¡ç®—é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„MSEæŸå¤±
#                 loss = criterion(pred, y_batch)
#
#                 loss.backward()
#                 optimizer.step()
#
#                 total_epoch_loss += loss.item()
#                 batch_count += 1
#
#         if (epoch + 1) % 10 == 0:
#             avg_epoch_loss = total_epoch_loss / batch_count if batch_count > 0 else 0.0
#             print(f"[Align] Epoch {epoch + 1:3d}/{epochs}, Avg Regression Loss: {avg_epoch_loss:.5f}")
#
#     return mlp_list


# train_eval.py

# å°†è¿™ä¸ªå‡½æ•°æ•´ä½“æ›¿æ¢æ‰
# def train_alignment_mlps(
#         drugvirus_feats,
#         mdad_feats,
#         device,
#         epochs: int = 200,
#         lr: float = 1e-3,
#         batch_size: int = 256,
# ):
#     """
#     ã€ä¿®æ”¹åã€‘: è¿™ä¸ªå‡½æ•°ç°åœ¨åªåˆ›å»ºä¸å¯¹é½çš„MLPæ¨¡å—ï¼Œä¸å†è¿›è¡Œè®­ç»ƒã€‚
#     è®­ç»ƒå°†åœ¨ä¸» EWC å¾ªç¯ä¸­è¿›è¡Œã€‚
#     """
#
#     def _make_mlp(in_dim, out_dim):
#         mlp = torch.nn.Sequential(
#             torch.nn.Linear(in_dim, (in_dim + out_dim) // 2),  # ä½¿ç”¨æ›´å¹³æ»‘çš„ä¸­é—´ç»´åº¦
#             torch.nn.ReLU(),
#             torch.nn.Dropout(0.2),
#             torch.nn.Linear((in_dim + out_dim) // 2, out_dim),
#         ).to(device)
#         # ç§»é™¤ eye_ åˆå§‹åŒ–ï¼Œè®©ç½‘ç»œä»éšæœºçŠ¶æ€å¼€å§‹å­¦ä¹ 
#         # torch.nn.init.eye_(mlp[-1].weight)
#         # torch.nn.init.zeros_(mlp[-1].bias)
#         return mlp
#
#     mlp_list = []
#     for dv_feat, mdad_feat in zip(drugvirus_feats, mdad_feats):
#         mlp = _make_mlp(dv_feat.shape[1], mdad_feat.shape[1])
#         mlp_list.append(mlp)
#
#     # å°†ç‹¬ç«‹çš„MLPåˆ—è¡¨å°è£…æˆä¸€ä¸ªnn.ModuleListï¼Œè¿™æ ·å®ƒä»¬å°±èƒ½è¢«PyTorchçš„ä¼˜åŒ–å™¨æ­£ç¡®è¯†åˆ«
#     alignment_mlps = nn.ModuleList(mlp_list).to(device)
#
#     # ç§»é™¤æ•´ä¸ªè®­ç»ƒå¾ªç¯ (for epoch in ...)ï¼Œç›´æ¥è¿”å›æœªè®­ç»ƒçš„MLPæ¨¡å—
#     print(f"æˆåŠŸåˆ›å»º {len(alignment_mlps)} ä¸ªå¯¹é½MLPæ¨¡å—ï¼ˆæœªè®­ç»ƒï¼‰ã€‚")
#     return alignment_mlps
# train_eval.py

# å°†è¿™ä¸ªå‡½æ•°æ•´ä½“æ›¿æ¢æ‰
def train_alignment_mlps(
        drugvirus_feats,
        mdad_feats,
        device,
        epochs: int = 200,
        lr: float = 1e-3,
        batch_size: int = 256,
):
    """
    ã€ä¿®æ”¹åã€‘: è¿™ä¸ªå‡½æ•°ç°åœ¨åªåˆ›å»ºMLPæ¨¡å—ï¼Œå¹¶å°†å…¶åˆå§‹åŒ–ä¸ºè¿‘ä¼¼æ’ç­‰æ˜ å°„ã€‚
    å®ƒä¸å†è¿›è¡Œé¢„è®­ç»ƒï¼Œå› ä¸ºè®­ç»ƒå°†åœ¨ä¸» EWC å¾ªç¯ä¸­è¿›è¡Œã€‚
    """

    def _make_mlp(in_dim, out_dim):
        # å¿…é¡»ç¡®ä¿è¾“å…¥å’Œè¾“å‡ºç»´åº¦ç›¸åŒæ‰èƒ½è¿›è¡Œæ’ç­‰åˆå§‹åŒ–
        if in_dim != out_dim:
            # å¦‚æœç»´åº¦ä¸åŒï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚ï¼Œä½†æ— æ³•åšæ’ç­‰åˆå§‹åŒ–ã€‚
            # è¿™åœ¨ä½ çš„æµç¨‹ä¸­ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå¤–éƒ¨å¯¹é½å·²ç»å¤„ç†äº†ç»´åº¦é—®é¢˜ã€‚
            print(f"è­¦å‘Š: åˆ›å»ºå¯¹é½MLPæ—¶è¾“å…¥({in_dim})å’Œè¾“å‡º({out_dim})ç»´åº¦ä¸åŒï¼Œæ— æ³•è¿›è¡Œæ’ç­‰åˆå§‹åŒ–ã€‚")
            mlp = torch.nn.Sequential(
                torch.nn.Linear(in_dim, out_dim)
            ).to(device)
        else:
            # ç»´åº¦ç›¸åŒæ—¶ï¼Œåˆ›å»ºå•å±‚çº¿æ€§ç½‘ç»œ
            mlp = torch.nn.Sequential(
                torch.nn.Linear(in_dim, out_dim)
            ).to(device)
            # æ ¸å¿ƒï¼šå°†æƒé‡åˆå§‹åŒ–ä¸ºå•ä½çŸ©é˜µï¼Œåç½®åˆå§‹åŒ–ä¸ºé›¶ï¼Œå®ç°æ’ç­‰æ˜ å°„
            print(f"æˆåŠŸ: åˆ›å»ºäº†ä¸€ä¸ªè¾“å…¥è¾“å‡ºç»´åº¦ä¸º {in_dim} çš„æ’ç­‰åˆå§‹åŒ–MLPã€‚")
            torch.nn.init.eye_(mlp[0].weight)
            torch.nn.init.zeros_(mlp[0].bias)

        return mlp

    mlp_list = []
    # æ³¨æ„ï¼šè¿™é‡Œçš„è¾“å…¥ç‰¹å¾ drugvirus_feats å®é™…ä¸Šæ˜¯å·²ç»ç»è¿‡å¤–éƒ¨å¯¹é½çš„ï¼Œ
    # æ‰€ä»¥å®ƒä»¬çš„ç»´åº¦åº”è¯¥å’Œ mdad_feats ç›¸åŒã€‚
    for dv_feat, mdad_feat in zip(drugvirus_feats, mdad_feats):
        # æˆ‘ä»¬ä½¿ç”¨mdad_featçš„ç»´åº¦ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼Œå› ä¸ºè¿™æ˜¯å¯¹é½åçš„ç›®æ ‡ç»´åº¦
        dim = mdad_feat.shape[1]
        mlp = _make_mlp(dim, dim)
        mlp_list.append(mlp)

    # å°†ç‹¬ç«‹çš„MLPåˆ—è¡¨å°è£…æˆä¸€ä¸ªnn.ModuleList
    alignment_mlps = nn.ModuleList(mlp_list).to(device)

    print(f"æˆåŠŸåˆ›å»º {len(alignment_mlps)} ä¸ªå¯¹é½MLPæ¨¡å—ï¼ˆå·²åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„ï¼‰ã€‚")
    return alignment_mlps




def build_gcn_features(Fd, Fm):#å°†å¾®ç”Ÿç‰©ä¸è¯ç‰©ç‰¹å¾å †å 
    # n_drug = Fd.shape[0]
    # n_microbe = Fm.shape[0]
    n_drug = Fd.shape[0]
    n_microbe = Fm.shape[0]
    zero_drug = np.zeros((n_drug, Fm.shape[1]))
    zero_microbe = np.zeros((n_microbe, Fd.shape[1]))
    top = np.hstack((zero_drug, Fd))
    bottom = np.hstack((Fm, zero_microbe))
    X = np.vstack((top, bottom))
    return X

# def train_gcn(train_data,  edge_index, edge_weight,drug_fg,drug_features,drug_bert, microbe_features,microbe_bert,microbe_path, microbe_offset,epochs=100, lr=0.01, hidden=64, dropout=0.5,
#               args=None,device='cpu'):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],  # ä¼ é€’åŸå§‹ç»´åº¦
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False, # åªå¯¹è¯ç‰©è¿›è¡Œ MLP
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr)
#     criterion = nn.BCEWithLogitsLoss()
#     model.train()
#     decoder.train()
#
#     # æˆ‘ä»¬éœ€è¦ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¢¯åº¦
#     last_epoch_gradients = None
#     # ====== è¿™ä¸‰è¡Œä¸€å®šè¦åŠ ä¸Šï¼======
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32).to(device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32).to(device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32).to(device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32).to(device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32).to(device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32).to(device)
#
#     # ============================
#     for epoch in range(epochs):
#         optimizer.zero_grad()# ğŸŒŸ æ¸…ç©ºæ¢¯åº¦
#         drug_idx, microbe_idx, labels = train_data
#
#         drug_fg = torch.tensor(drug_fg, dtype=torch.float32).to(device)
#         #microbe_feat = torch.tensor(microbe_features, dtype=torch.float32).to(device)
#
#         #adj = torch.tensor(A, dtype=torch.float32).to(device)
#
#         # è·å–GCNåµŒå…¥
#         embeddings, X = model((drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                               edge_index, edge_weight)
#
#         # ã€å…³é”®ä¿®æ”¹1ã€‘: å‘Šè¯‰PyTorchæˆ‘ä»¬éœ€è¦è®¡ç®—è¿™ä¸ªä¸­é—´å˜é‡çš„æ¢¯åº¦
#         #embeddings.requires_grad_(True)
#         embeddings.retain_grad()
#
#         drug_emb = embeddings[drug_idx]
#         microbe_emb = embeddings[microbe_offset + microbe_idx]#åŸæœ¬è¿™é‡Œæ˜¯1180
#         logits = decoder(drug_emb, microbe_emb)
#         loss = criterion(logits, torch.tensor(labels, dtype=torch.float32).to(device))
#
#         loss.backward()
#
#         # ã€å…³é”®ä¿®æ”¹2ã€‘: åœ¨ä¼˜åŒ–å™¨æ›´æ–°å‰ï¼Œä¿å­˜æ¢¯åº¦
#         if epoch == epochs - 1:
#             last_epoch_gradients = embeddings.grad.detach().cpu().numpy()
#
#         optimizer.step()
#
#         if (epoch + 1) % 40 == 0:
#             print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}")
#
#     # ã€å…³é”®ä¿®æ”¹3ã€‘: è¿”å›æ¨¡å‹ã€è§£ç å™¨å’Œæ¢¯åº¦
#     return model, decoder, last_epoch_gradients,embeddings,X
#
# def train_gcn(
#     train_data, edge_index, edge_weight,
#     drug_fg, drug_features, drug_bert,
#     microbe_features, microbe_bert, microbe_path, microbe_offset,
#     epochs=100, lr=0.01, hidden=64, dropout=0.5,
#     args=None, device='cpu', batch_size=256  # æ–°å¢ batch_size å‚æ•°ï¼Œé»˜è®¤256
# ):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],  # ä¼ é€’åŸå§‹ç»´åº¦
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,  # åªå¯¹è¯ç‰©è¿›è¡Œ MLP
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr)
#     criterion = nn.BCEWithLogitsLoss()
#     model.train()
#     decoder.train()
#
#     last_epoch_gradients = None
#
#     # ====== è¿™å‡ è¡Œåªéœ€è½¬æ¢ä¸€æ¬¡ï¼======
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32).to(device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32).to(device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32).to(device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32).to(device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32).to(device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32).to(device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#
#     num_samples = len(drug_idx)
#
#
#     for epoch in range(epochs):
#         # â¬…ï¸ æ–°å¢ï¼šæ¯40è½®èµ·å§‹æ—¶é—´
#         if epoch % 40 == 0:
#             start_time_40 = time.time()
#
#         model.train()
#         decoder.train()
#         permutation = np.random.permutation(num_samples)
#         total_loss = 0.0
#
#         for i in range(0, num_samples, batch_size):
#             idx = permutation[i:i+batch_size]
#             batch_drug_idx = drug_idx[idx]
#             batch_microbe_idx = microbe_idx[idx]
#             batch_labels = labels[idx]
#
#             optimizer.zero_grad()
#
#             # è·å–GCNåµŒå…¥ï¼ˆå…¨å›¾ç‰¹å¾ï¼Œbatché‡‡æ ·è¾¹ï¼‰
#             embeddings, X = model(
#                 (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                 edge_index, edge_weight
#             )
#             embeddings.retain_grad()
#
#             drug_emb = embeddings[batch_drug_idx]
#             microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#             logits = decoder(drug_emb, microbe_emb)
#             loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32).to(device))
#             loss.backward()
#
#             # åªä¿å­˜æœ€åä¸€ä¸ªepochæœ€åä¸€ä¸ªbatchçš„æ¢¯åº¦
#             if epoch == epochs - 1 and i + batch_size >= num_samples:
#                 last_epoch_gradients = embeddings.grad.detach().cpu().numpy()
#
#             optimizer.step()
#             total_loss += loss.item()
#
#         if (epoch + 1) % 40 == 0:
#             end_time_40 = time.time()
#             elapsed_40 = end_time_40 - start_time_40
#             avg_loss = total_loss / (num_samples // batch_size + int(num_samples % batch_size != 0))
#             print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, 40-epoch time: {elapsed_40:.2f} sec")
#
#     return model, decoder, last_epoch_gradients, embeddings, X
#

# train_eval.py

import time  # ç¡®ä¿å¯¼å…¥äº† time æ¨¡å—

#
# def train_gcn(
#         train_data, edge_index, edge_weight,
#         drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs=100, lr=0.01, hidden=64, dropout=0.5,
#         args=None, device='cpu', batch_size=256
# ):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr)
#     criterion = nn.BCEWithLogitsLoss()
#
#     # ç‰¹å¾è½¬æ¢åªéœ€è¦ä¸€æ¬¡
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#     num_samples = len(drug_idx)
#
#     last_epoch_gradients = None
#     final_embeddings = None
#     final_X = None
#
#     for epoch in range(epochs):
#         if epoch % 40 == 0:
#             start_time_40 = time.time()
#
#         model.train()
#         decoder.train()
#
#         # ======================= æ ¸å¿ƒä¼˜åŒ–ç‚¹ =======================
#         # åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶ï¼Œè®¡ç®—ä¸€æ¬¡å…¨å›¾çš„ GCN åµŒå…¥
#         optimizer.zero_grad()
#         embeddings, X = model(
#             (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#             edge_index, edge_weight
#         )
#         embeddings.retain_grad()  # ä¾ç„¶éœ€è¦ä¿ç•™æ¢¯åº¦
#         # =========================================================
#
#         permutation = np.random.permutation(num_samples)
#         total_loss = 0.0
#
#         for i in range(0, num_samples, batch_size):
#             idx = permutation[i:i + batch_size]
#             batch_drug_idx = drug_idx[idx]
#             batch_microbe_idx = microbe_idx[idx]
#             batch_labels = labels[idx]
#
#             # ç›´æ¥ä»é¢„å…ˆè®¡ç®—å¥½çš„ embeddings ä¸­å–å€¼ï¼Œä¸å†é‡æ–°è®¡ç®—GCN
#             drug_emb = embeddings[batch_drug_idx]
#             microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#
#             logits = decoder(drug_emb, microbe_emb)
#             loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
#
#             # åœ¨ batch å¾ªç¯ä¸­ç´¯åŠ æŸå¤±ï¼Œåœ¨ epoch ç»“æŸåç»Ÿä¸€åå‘ä¼ æ’­
#             total_loss += loss
#
#         # åœ¨æ‰€æœ‰ batch çš„æŸå¤±ç´¯åŠ å®Œåï¼Œè¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­å’Œä¼˜åŒ–
#         total_loss.backward()
#         optimizer.step()
#
#         if epoch == epochs - 1:
#             last_epoch_gradients = embeddings.grad.detach().cpu().numpy() if embeddings.grad is not None else None
#             final_embeddings = embeddings.detach()
#             final_X = X.detach()
#
#         if (epoch + 1) % 40 == 0:
#             end_time_40 = time.time()
#             elapsed_40 = end_time_40 - start_time_40
#             avg_loss = total_loss.item() / (num_samples // batch_size + int(num_samples % batch_size != 0))
#             print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, 40-epoch time: {elapsed_40:.2f} sec")
#
#     return model, decoder, last_epoch_gradients, final_embeddings, final_X
#
# ===================================================================

# =================================================================================
#  è¯·ç”¨ä»¥ä¸‹å®Œæ•´å‡½æ•°æ›¿æ¢æ‚¨æ–‡ä»¶ä¸­æ—§çš„ train_gcn å‡½æ•°
# =================================================================================
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
# ç¡®ä¿ evaluate_gcn å‡½æ•°åœ¨å½“å‰æ–‡ä»¶ä¸­å¯ç”¨ï¼Œæˆ–è€…å·²ä»åˆ«å¤„æ­£ç¡®å¯¼å…¥
# ä¾‹å¦‚: from .train_eval import evaluate_gcn
from gcn_model import GCNWithMLP, MLPDecoder


# def train_gcn(
#         train_data, edge_index, edge_weight,
#         drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs=100, lr=0.01, hidden=64, dropout=0.5,
#         args=None, device='cpu', batch_size=256,
#         test_data=None  # <--- ç¬¬1å¤„ä¿®æ”¹ï¼šå¢åŠ  test_data å‚æ•°
# ):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr)
#     criterion = nn.BCEWithLogitsLoss()
#
#     # ç‰¹å¾è½¬æ¢åªéœ€è¦ä¸€æ¬¡
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#     num_samples = len(drug_idx)
#
#     last_epoch_gradients = None
#     final_embeddings = None
#     final_X = None
#
#     for epoch in range(epochs):
#         if epoch % 40 == 0:
#             start_time_40 = time.time()
#
#         model.train()
#         decoder.train()
#
#         # ======================= æ ¸å¿ƒä¼˜åŒ–ç‚¹ (ä¿æŒä¸å˜) =======================
#         optimizer.zero_grad()
#         embeddings, X = model(
#             (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#             edge_index, edge_weight
#         )
#         embeddings.retain_grad()
#         # ====================================================================
#
#         permutation = np.random.permutation(num_samples)
#         total_loss = 0.0
#
#         for i in range(0, num_samples, batch_size):
#             idx = permutation[i:i + batch_size]
#             batch_drug_idx = drug_idx[idx]
#             batch_microbe_idx = microbe_idx[idx]
#             batch_labels = labels[idx]
#
#             drug_emb = embeddings[batch_drug_idx]
#             microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#
#             logits = decoder(drug_emb, microbe_emb)
#             loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
#
#             total_loss += loss
#
#         total_loss.backward()
#         optimizer.step()
#
#         if epoch == epochs - 1:
#             last_epoch_gradients = embeddings.grad.detach().cpu().numpy() if embeddings.grad is not None else None
#             final_embeddings = embeddings.detach()
#             final_X = X.detach()
#
#         # ======================= ç¬¬2å¤„ä¿®æ”¹ï¼šåœ¨æ­¤å¤„å¢åŠ è¯„ä¼°é€»è¾‘ =======================
#         if (epoch + 1) % 40 == 0:
#             end_time_40 = time.time()
#             elapsed_40 = end_time_40 - start_time_40
#             avg_loss = total_loss.item() / (num_samples / batch_size)
#
#             # å…ˆæ„å»ºåŸºç¡€çš„è¾“å‡ºå­—ç¬¦ä¸²
#             output_string = f"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}, 40-epoch time: {elapsed_40:.2f} sec"
#
#             # å¦‚æœæœ‰æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œè¯„ä¼°å¹¶è¿½åŠ ç»“æœåˆ°å­—ç¬¦ä¸²
#             if test_data is not None:
#                 model.eval()
#                 decoder.eval()
#
#                 with torch.no_grad():
#                     test_auc, test_aupr = evaluate_gcn(
#                         model, decoder, test_data, edge_index, edge_weight,
#                         drug_fg, drug_features, drug_bert,
#                         microbe_features, microbe_bert, microbe_path,
#                         microbe_offset, device
#                     )
#
#                 # ä½¿ç”¨ += æ¥è¿½åŠ æµ‹è¯•ç»“æœ
#                 output_string += f", Test AUC: {test_auc:.4f}, Test AUPR: {test_aupr:.4f}"
#
#             # æœ€åï¼Œåªç”¨ä¸€ä¸ª print è¯­å¥è¾“å‡ºæ‰€æœ‰ä¿¡æ¯
#             print(output_string)
#
#             # åœ¨è¿™ä¸ªä»£ç å—ç»“æŸåï¼Œä¸‹ä¸€æ¬¡å¾ªç¯å¼€å§‹æ—¶ï¼Œ
#             # model.train() å’Œ decoder.train() ä¼šè¢«è‡ªåŠ¨è°ƒç”¨ï¼Œæ— éœ€æ‰‹åŠ¨åˆ‡æ¢å›æ¥ã€‚
#         # ========================================================================
#
#     return model, decoder, last_epoch_gradients, final_embeddings, final_X

# train_eval.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
import os
import matplotlib.pyplot as plt
from gcn_model import GCNWithMLP, MLPDecoder


# def train_gcn(
#         train_data, edge_index, edge_weight,
#         drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs=100, lr=0.01, hidden=64, dropout=0.5,
#         args=None, device='cpu', batch_size=256,
#         test_data=None,
#         fold_num=0,
#         save_dir='.',
#         plot_filename=None , # <--- ã€æ ¸å¿ƒä¿®æ”¹1ã€‘: æ–°å¢ plot_filename å‚æ•°
#         weight_decay = 0.0 , # <--- ã€æ ¸å¿ƒä¿®æ”¹1ï¼šå¢åŠ å‚æ•°ã€‘
# # --- ã€æ–°å¢ã€‘æ—©åœç›¸å…³å‚æ•° ---
#         use_early_stopping=False,
#         patience=50
# ):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr,weight_decay=args.wd_retrain )
#     # ======================== ã€åœ¨è¿™é‡Œæ–°å¢ã€‘ ========================
#     # 1. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
#     scheduler = torch.optim.lr_scheduler.StepLR(
#         optimizer,
#         step_size=args.lr_step_size,
#         gamma=args.lr_gamma
#     )
#     # ===============================================================
#
#     criterion = nn.BCEWithLogitsLoss()
#
#     # ç‰¹å¾è½¬æ¢åªéœ€è¦ä¸€æ¬¡
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#     num_samples = len(drug_idx)
#
#     last_epoch_gradients = None
#     final_embeddings = None
#     final_X = None
#
#     plot_epochs = []
#     plot_aucs = []
#     plot_auprs = []
#
#     # --- ã€æ–°å¢ã€‘æ—©åœå˜é‡åˆå§‹åŒ– ---
#     if use_early_stopping and args.early_stopping_patience > 0:
#         print(f"æç¤º: æ—©åœå·²å¯ç”¨ï¼Œè€å¿ƒå€¼ä¸º {patience} ä¸ª epochsã€‚")
#         best_auc = 0.0
#         epochs_no_improve = 0
#         best_model_state_dict = None
#         best_decoder_state_dict = None
#     # -----------------------------
#
#     for epoch in range(epochs):
#         if (epoch + 1) % 100 == 0:
#             start_time_20 = time.time()
#
#         model.train()
#         decoder.train()
#
#         optimizer.zero_grad()
#         embeddings, X = model(
#             (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#             edge_index, edge_weight
#         )
#         embeddings.retain_grad()
#
#         permutation = np.random.permutation(num_samples)
#         total_loss = 0.0
#
#         for i in range(0, num_samples, batch_size):
#             idx = permutation[i:i + batch_size]
#             batch_drug_idx = drug_idx[idx]
#             batch_microbe_idx = microbe_idx[idx]
#             batch_labels = labels[idx]
#             drug_emb = embeddings[batch_drug_idx]
#             microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#             logits = decoder(drug_emb, microbe_emb)
#             loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
#             total_loss += loss
#
#         total_loss.backward()
#         optimizer.step()
#         # ======================== ã€åœ¨è¿™é‡Œæ–°å¢ã€‘ ========================
#         # 2. åœ¨æ¯ä¸ªepochç»“æŸåï¼Œæ›´æ–°å­¦ä¹ ç‡
#         scheduler.step()
#         # ===============================================================
#
#         if epoch == epochs - 1:
#             last_epoch_gradients = embeddings.grad.detach().cpu().numpy() if embeddings.grad is not None else None
#             final_embeddings = embeddings.detach()
#             final_X = X.detach()
#
#         if (epoch + 1) % 6 == 0:
#             if test_data is not None:
#                 model.eval()
#                 decoder.eval()
#                 with torch.no_grad():
#                     test_auc, test_aupr = evaluate_gcn(
#                         model, decoder, test_data, edge_index, edge_weight,
#                         drug_fg, drug_features, drug_bert,
#                         microbe_features, microbe_bert, microbe_path,
#                         microbe_offset, device
#                     )
#                 plot_epochs.append(epoch + 1)
#                 plot_aucs.append(test_auc)
#                 plot_auprs.append(test_aupr)
#
#                 # --- ã€å°†ä½ çš„æ—©åœä»£ç å—ç²˜è´´åœ¨è¿™é‡Œã€‘ ---
#                 if use_early_stopping and patience > 0:
#                     if test_auc > best_auc:
#                         best_auc = test_auc
#                         epochs_no_improve = 0
#                         # ä½¿ç”¨ deepcopy ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
#                         best_model_state_dict = copy.deepcopy(model.state_dict())
#                         best_decoder_state_dict = copy.deepcopy(decoder.state_dict())
#                     else:
#                         epochs_no_improve += 1
#
#                     if epochs_no_improve >= patience:
#                         print(f"æ—©åœè§¦å‘: åœ¨ {epoch + 1} ä¸ª epochs åï¼ŒéªŒè¯é›† AUC è¿ç»­ {patience} æ¬¡æœªæå‡ã€‚")
#                         break  # é€€å‡ºè®­ç»ƒå¾ªç¯
#                 # ------------------------------------
#
#
#                 model.train()
#                 decoder.train()
#
#         if (epoch + 1) % 100 == 0:
#             end_time_20 = time.time()
#             elapsed_20 = end_time_20 - start_time_20
#             avg_loss = total_loss.item() / (num_samples / batch_size)
#             output_string = f"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}, 20-epoch time: {elapsed_20:.2f} sec"
#             if test_data is not None:
#                 model.eval()
#                 decoder.eval()
#                 with torch.no_grad():
#                     test_auc, test_aupr = evaluate_gcn(
#                         model, decoder, test_data, edge_index, edge_weight,
#                         drug_fg, drug_features, drug_bert,
#                         microbe_features, microbe_bert, microbe_path,
#                         microbe_offset, device
#                     )
#                 output_string += f", Test AUC: {test_auc:.4f}, Test AUPR: {test_aupr:.4f}"
#             print(output_string)
#
#         # --- ã€æ–°å¢ã€‘åŠ è½½æœ€ä½³æ¨¡å‹ ---
#     if use_early_stopping and args.early_stopping_patience > 0 and best_model_state_dict is not None:
#         print(f"åŠ è½½æ—©åœæ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ (Test AUC: {best_auc:.4f})")
#         model.load_state_dict(best_model_state_dict)
#         decoder.load_state_dict(best_decoder_state_dict)
#     # ---------------------------
#
#
#     # <--- ã€æ ¸å¿ƒä¿®æ”¹2ã€‘: ä¿®æ”¹ç»˜å›¾å’Œä¿å­˜é€»è¾‘ --->
#     if test_data is not None and plot_epochs and plot_filename:
#         # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡ç¼ºå¤±é—®é¢˜
#         plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
#         plt.rcParams['axes.unicode_minus'] = False
#
#         plt.figure(figsize=(12, 8))
#         plt.plot(plot_epochs, plot_aucs, marker='o', linestyle='-', label='Test AUC')
#         plt.plot(plot_epochs, plot_auprs, marker='s', linestyle='--', label='Test AUPR')
#         # æ–‡ä»¶åå·²ç»èƒ½åŒºåˆ†foldå’Œé˜¶æ®µï¼Œæ ‡é¢˜å¯ä»¥æ›´é€šç”¨æˆ–ä¹ŸåŒ…å«è¿™äº›ä¿¡æ¯
#         plt.title(f'Training Curve ({os.path.splitext(plot_filename)[0]})', fontsize=16)
#         plt.xlabel('Epoch', fontsize=12)
#         plt.ylabel('Score', fontsize=12)
#         plt.legend(fontsize=12)
#         plt.grid(True)
#
#         os.makedirs(save_dir, exist_ok=True)
#         # ä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶åæ„é€ å®Œæ•´è·¯å¾„
#         save_path = os.path.join(save_dir, plot_filename)
#         plt.savefig(save_path, dpi=300)
#         plt.close()
#         print(f"æˆåŠŸ: è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ -> {save_path}")
#     # <--- ã€æ ¸å¿ƒä¿®æ”¹2 ç»“æŸã€‘ --->
#
#     return model, decoder, last_epoch_gradients, final_embeddings, final_X
#


# def train_gcn(
#         train_data, edge_index, edge_weight,
#         drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs=100, lr=0.01, hidden=64, dropout=0.5,
#         args=None, device='cpu', batch_size=256,
#         test_data=None,
#         fold_num=0,
#         save_dir='.',
#         plot_filename=None,
#         weight_decay=0.0,
#         use_early_stopping=False,
#         patience=50
# ):
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr, weight_decay=args.wd_retrain)
#     scheduler = torch.optim.lr_scheduler.StepLR(
#         optimizer,
#         step_size=args.lr_step_size,
#         gamma=args.lr_gamma
#     )
#     criterion = nn.BCEWithLogitsLoss()
#
#     # ==================== ã€æ ¸å¿ƒä¿®æ”¹ï¼šåˆå§‹åŒ–æ··åˆç²¾åº¦ç»„ä»¶ã€‘ ====================
#     # ä»…å½“æ•°æ®é›†ä¸º aBiofilm ä¸”åœ¨ CUDA ä¸Šè¿è¡Œæ—¶å¯ç”¨ï¼Œç°åœ¨åŠ ä¸ŠMDAD
#     use_amp = (args.dataset in ['aBiofilm', 'MDAD']) and ('cuda' in str(device))
#
#     scaler = GradScaler(enabled=use_amp)
#     if use_amp:
#         print(f"æç¤º: å·²ä¸º aBiofilm æ•°æ®é›†å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (Fold {fold_num + 1})ã€‚")
#     # ========================================================================
#
#     # ç‰¹å¾è½¬æ¢åªéœ€è¦ä¸€æ¬¡
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#     num_samples = len(drug_idx)
#
#     last_epoch_gradients = None
#     final_embeddings = None
#     final_X = None
#
#     plot_epochs = []
#     plot_aucs = []
#     plot_auprs = []
#
#     if use_early_stopping and args.early_stopping_patience > 0:
#         print(f"æç¤º: æ—©åœå·²å¯ç”¨ï¼Œè€å¿ƒå€¼ä¸º {patience} ä¸ª epochsã€‚")
#         best_auc = 0.0
#         epochs_no_improve = 0
#         best_model_state_dict = None
#         best_decoder_state_dict = None
#
#     for epoch in range(epochs):
#         if (epoch + 1) % 100 == 0:
#             start_time_20 = time.time()
#
#         model.train()
#         decoder.train()
#
#         optimizer.zero_grad()
#
#         # ==================== ã€æ ¸å¿ƒä¿®æ”¹ï¼šåº”ç”¨æ··åˆç²¾åº¦è®­ç»ƒã€‘ ====================
#         # ä½¿ç”¨ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨åŒ…è£¹å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
#         with autocast(enabled=use_amp):
#             embeddings, X = model(
#                 (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                 edge_index, edge_weight
#             )
#             embeddings.retain_grad()
#
#             permutation = np.random.permutation(num_samples)
#             total_loss = 0.0
#
#             for i in range(0, num_samples, batch_size):
#                 idx = permutation[i:i + batch_size]
#                 batch_drug_idx = drug_idx[idx]
#                 batch_microbe_idx = microbe_idx[idx]
#                 batch_labels = labels[idx]
#                 drug_emb = embeddings[batch_drug_idx]
#                 microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#                 logits = decoder(drug_emb, microbe_emb)
#                 loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
#                 total_loss += loss
#
#         # ä½¿ç”¨ GradScaler ç¼©æ”¾æŸå¤±ã€åå‘ä¼ æ’­å’Œæ›´æ–°ä¼˜åŒ–å™¨
#         scaler.scale(total_loss).backward()
#         scaler.step(optimizer)
#         scaler.update()
#         # ========================================================================
#
#         scheduler.step()
#
#         if epoch == epochs - 1:
#             last_epoch_gradients = embeddings.grad.detach().cpu().numpy() if embeddings.grad is not None else None
#             final_embeddings = embeddings.detach()
#             final_X = X.detach()
#
#         if (epoch + 1) % 6 == 0:
#             if test_data is not None:
#                 model.eval()
#                 decoder.eval()
#                 with torch.no_grad():
#                     test_auc, test_aupr = evaluate_gcn(
#                         model, decoder, test_data, edge_index, edge_weight,
#                         drug_fg, drug_features, drug_bert,
#                         microbe_features, microbe_bert, microbe_path,
#                         microbe_offset, device
#                     )
#                 plot_epochs.append(epoch + 1)
#                 plot_aucs.append(test_auc)
#                 plot_auprs.append(test_aupr)
#
#                 if use_early_stopping and patience > 0:
#                     if test_auc > best_auc:
#                         best_auc = test_auc
#                         epochs_no_improve = 0
#                         best_model_state_dict = copy.deepcopy(model.state_dict())
#                         best_decoder_state_dict = copy.deepcopy(decoder.state_dict())
#                     else:
#                         epochs_no_improve += 1
#
#                     if epochs_no_improve >= patience:
#                         print(f"æ—©åœè§¦å‘: åœ¨ {epoch + 1} ä¸ª epochs åï¼ŒéªŒè¯é›† AUC è¿ç»­ {patience} æ¬¡æœªæå‡ã€‚")
#                         break
#                 model.train()
#                 decoder.train()
#
#         if (epoch + 1) % 100 == 0:
#             end_time_20 = time.time()
#             elapsed_20 = end_time_20 - start_time_20
#             avg_loss = total_loss.item() / (num_samples / batch_size)
#             output_string = f"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}, 20-epoch time: {elapsed_20:.2f} sec"
#             if test_data is not None:
#                 model.eval()
#                 decoder.eval()
#                 with torch.no_grad():
#                     test_auc, test_aupr = evaluate_gcn(
#                         model, decoder, test_data, edge_index, edge_weight,
#                         drug_fg, drug_features, drug_bert,
#                         microbe_features, microbe_bert, microbe_path,
#                         microbe_offset, device
#                     )
#                 output_string += f", Test AUC: {test_auc:.4f}, Test AUPR: {test_aupr:.4f}"
#             print(output_string)
#
#     if use_early_stopping and args.early_stopping_patience > 0 and best_model_state_dict is not None:
#         print(f"åŠ è½½æ—©åœæ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ (Test AUC: {best_auc:.4f})")
#         model.load_state_dict(best_model_state_dict)
#         decoder.load_state_dict(best_decoder_state_dict)
#
#     if test_data is not None and plot_epochs and plot_filename:
#         plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
#         plt.rcParams['axes.unicode_minus'] = False
#         plt.figure(figsize=(12, 8))
#         plt.plot(plot_epochs, plot_aucs, marker='o', linestyle='-', label='Test AUC')
#         plt.plot(plot_epochs, plot_auprs, marker='s', linestyle='--', label='Test AUPR')
#         plt.title(f'Training Curve ({os.path.splitext(plot_filename)[0]})', fontsize=16)
#         plt.xlabel('Epoch', fontsize=12)
#         plt.ylabel('Score', fontsize=12)
#         plt.legend(fontsize=12)
#         plt.grid(True)
#         os.makedirs(save_dir, exist_ok=True)
#         save_path = os.path.join(save_dir, plot_filename)
#         plt.savefig(save_path, dpi=300)
#         plt.close()
#         print(f"æˆåŠŸ: è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ -> {save_path}")
#
#     return model, decoder, last_epoch_gradients, final_embeddings, final_X
#
#


# train_eval.py

# ... (æ–‡ä»¶é¡¶éƒ¨çš„ import ä¿æŒä¸å˜) ...

# è¯·ç”¨ä¸‹é¢çš„å‡½æ•°ã€æ•´ä½“æ›¿æ¢ã€‘ä½ æ–‡ä»¶ä¸­æ—§çš„ train_gcn å‡½æ•°

def train_gcn(
        train_data, edge_index, edge_weight,
        drug_fg, drug_features, drug_bert,
        microbe_features, microbe_bert, microbe_path, microbe_offset,
        epochs=100, lr=0.01, hidden=64, dropout=0.5,
        args=None, device='cpu', batch_size=256,
        test_data=None,
        fold_num=0,
        save_dir='.',
        plot_filename=None,
        weight_decay=0.0,
        use_early_stopping=False,
        patience=50
):
    torch.autograd.set_detect_anomaly(True)  # <--- åœ¨è¿™é‡Œæ·»åŠ 
    model = GCNWithMLP(
        drug_in_dim=drug_fg.shape[1],
        drug_out_dim=drug_fg.shape[0],
        microbe_dim=microbe_features.shape[1],
        microbe_out_dim=microbe_features.shape[1],
        gcn_hidden=hidden,
        dropout=dropout,
        use_microbe_mlp=False,
        dataset_name=args.dataset
    ).to(device)

    decoder = MLPDecoder(hidden).to(device)
    optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr, weight_decay=args.wd_retrain)
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=args.lr_step_size,
        gamma=args.lr_gamma
    )
    criterion = nn.BCEWithLogitsLoss()

   # use_amp = (args.dataset in ['a']) and ('cuda' in str(device))
    use_amp = True  # å½»åº•å…³é—­æ··åˆç²¾åº¦
    scaler = GradScaler(enabled=use_amp)
    if use_amp:
        print(f"æç¤º: å·²ä¸º {args.dataset} æ•°æ®é›†å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (Fold {fold_num + 1})ã€‚")
    else:
        print('ä¸å¯ç”¨æ··åˆç²¾åº¦')

    drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
    drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
    drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
    microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
    microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
    microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)

    drug_idx, microbe_idx, labels = train_data
    drug_idx = np.array(drug_idx)
    microbe_idx = np.array(microbe_idx)
    labels = np.array(labels)
    num_samples = len(drug_idx)

    last_epoch_gradients = None
    final_embeddings = None
    final_X = None

    plot_epochs = []
    plot_aucs = []
    plot_auprs = []
    plot_accs = [] # æ–°å¢

    if use_early_stopping and args.early_stopping_patience > 0:
        print(f"æç¤º: æ—©åœå·²å¯ç”¨ï¼Œè€å¿ƒå€¼ä¸º {patience} ä¸ª epochsã€‚")
        best_auc = 0.0
        epochs_no_improve = 0
        best_model_state_dict = None
        best_decoder_state_dict = None

    for epoch in range(epochs):
        if (epoch + 1) % 100 == 0:
            start_time_20 = time.time()

        model.train()
        decoder.train()
        optimizer.zero_grad()

        with autocast(enabled=use_amp):
            embeddings, X = model(
                (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
                edge_index, edge_weight
            )
            embeddings.retain_grad()
            permutation = np.random.permutation(num_samples)
            total_loss = 0.0
            for i in range(0, num_samples, batch_size):
                idx = permutation[i:i + batch_size]
                batch_drug_idx = drug_idx[idx]
                batch_microbe_idx = microbe_idx[idx]
                batch_labels = labels[idx]
                drug_emb = embeddings[batch_drug_idx]
                microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
                logits = decoder(drug_emb, microbe_emb)
                loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
                total_loss += loss

        scaler.scale(total_loss).backward()

        # --- æ·»åŠ æ¢¯åº¦è£å‰ª ---
        # åœ¨ optimizer.step() ä¹‹å‰ unscale æ¢¯åº¦
        scaler.unscale_(optimizer)
        # å¯¹ unscale åçš„æ¢¯åº¦è¿›è¡Œè£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        # --------------------


        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        if epoch == epochs - 1:
            last_epoch_gradients = embeddings.grad.detach().cpu().numpy() if embeddings.grad is not None else None
            final_embeddings = embeddings.detach()
            final_X = X.detach()

        if (epoch + 1) % 6 == 0:
            if test_data is not None:
                model.eval()
                decoder.eval()
                with torch.no_grad():
                    # --- ã€ä¿®æ”¹ç‚¹ 1ã€‘ ---
                    test_auc, test_aupr, test_acc = evaluate_gcn(
                        model, decoder, test_data, edge_index, edge_weight,
                        drug_fg, drug_features, drug_bert,
                        microbe_features, microbe_bert, microbe_path,
                        microbe_offset, device
                    )
                plot_epochs.append(epoch + 1)
                plot_aucs.append(test_auc)
                plot_auprs.append(test_aupr)
                plot_accs.append(test_acc) # æ–°å¢

                if use_early_stopping and patience > 0:
                    if test_auc > best_auc:
                        best_auc = test_auc
                        epochs_no_improve = 0
                        best_model_state_dict = copy.deepcopy(model.state_dict())
                        best_decoder_state_dict = copy.deepcopy(decoder.state_dict())
                    else:
                        epochs_no_improve += 1

                    if epochs_no_improve >= patience:
                        print(f"æ—©åœè§¦å‘: åœ¨ {epoch + 1} ä¸ª epochs åï¼ŒéªŒè¯é›† AUC è¿ç»­ {patience} æ¬¡æœªæå‡ã€‚")
                        break
                model.train()
                decoder.train()

        if (epoch + 1) % 100 == 0:
            end_time_20 = time.time()
            elapsed_20 = end_time_20 - start_time_20
            avg_loss = total_loss.item() / (num_samples / batch_size)
            output_string = f"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_loss:.4f}, 20-epoch time: {elapsed_20:.2f} sec"
            if test_data is not None:
                model.eval()
                decoder.eval()
                with torch.no_grad():
                    # --- ã€ä¿®æ”¹ç‚¹ 2ã€‘ ---
                    test_auc, test_aupr, test_acc = evaluate_gcn(
                        model, decoder, test_data, edge_index, edge_weight,
                        drug_fg, drug_features, drug_bert,
                        microbe_features, microbe_bert, microbe_path,
                        microbe_offset, device
                    )
                # --- ã€ä¿®æ”¹ç‚¹ 3ã€‘ ---
                output_string += f", Test AUC: {test_auc:.4f}, AUPR: {test_aupr:.4f}, ACC: {test_acc:.4f}"
            print(output_string)

    if use_early_stopping and args.early_stopping_patience > 0 and best_model_state_dict is not None:
        print(f"åŠ è½½æ—©åœæ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ (Test AUC: {best_auc:.4f})")
        model.load_state_dict(best_model_state_dict)
        decoder.load_state_dict(best_decoder_state_dict)

    if test_data is not None and plot_epochs and plot_filename:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.figure(figsize=(12, 8))
        plt.plot(plot_epochs, plot_aucs, marker='o', linestyle='-', label='Test AUC')
        plt.plot(plot_epochs, plot_auprs, marker='s', linestyle='--', label='Test AUPR')
        plt.plot(plot_epochs, plot_accs, marker='^', linestyle=':', label='Test ACC') # æ–°å¢
        plt.title(f'Training Curve ({os.path.splitext(plot_filename)[0]})', fontsize=16)
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Score', fontsize=12)
        plt.legend(fontsize=12)
        plt.grid(True)
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, plot_filename)
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"æˆåŠŸ: è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ -> {save_path}")

    return model, decoder, last_epoch_gradients, final_embeddings, final_X


import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import time

# å‡è®¾ GCNWithMLP å’Œ MLPDecoder ç±»å·²ç»å®šä¹‰å¥½

# def train_gcn(
#         train_data, edge_index, edge_weight,
#         drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs=100, lr=0.01, hidden=64, dropout=0.5,
#         args=None, device='cpu', batch_size=256
# ):
#     """
#     ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆMini-Batch SGDï¼‰è®­ç»ƒGCNæ¨¡å‹ã€‚
#     æ¯ä¸ªbatchéƒ½ä¼šæ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„å‰å‘ã€åå‘å’Œæ›´æ–°æ­¥éª¤ã€‚
#     """
#     model = GCNWithMLP(
#         drug_in_dim=drug_fg.shape[1],
#         drug_out_dim=drug_fg.shape[0],
#         microbe_dim=microbe_features.shape[1],
#         microbe_out_dim=microbe_features.shape[1],
#         gcn_hidden=hidden,
#         dropout=dropout,
#         use_microbe_mlp=False,
#         dataset_name=args.dataset
#     ).to(device)
#
#     decoder = MLPDecoder(hidden).to(device)
#     optimizer = optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=lr)
#     criterion = nn.BCEWithLogitsLoss()
#
#     # ç‰¹å¾è½¬æ¢åªéœ€è¦ä¸€æ¬¡
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     drug_idx, microbe_idx, labels = train_data
#     drug_idx = np.array(drug_idx)
#     microbe_idx = np.array(microbe_idx)
#     labels = np.array(labels)
#     num_samples = len(drug_idx)
#
#     for epoch in range(epochs):
#         if epoch % 40 == 0:
#             start_time_40 = time.time()
#
#         model.train()
#         decoder.train()
#
#         permutation = np.random.permutation(num_samples)
#         epoch_loss = 0.0  # ç”¨äºè®°å½•å’Œæ‰“å°å½“å‰epochçš„æ€»æŸå¤±
#
#         for i in range(0, num_samples, batch_size):
#             # 1. æ¸…ç©ºä¸Šä¸€è½®çš„æ¢¯åº¦
#             optimizer.zero_grad()
#
#             # 2. å¯¹æ•´ä¸ªå›¾è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„åµŒå…¥
#             #    æ³¨æ„ï¼šè¿™ä¸€æ­¥åœ¨æ¯ä¸ªbatchéƒ½ä¼šæ‰§è¡Œï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜
#             embeddings, X = model(
#                 (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                 edge_index, edge_weight
#             )
#
#             # 3. è·å–å½“å‰batchçš„æ•°æ®
#             idx = permutation[i:i + batch_size]
#             batch_drug_idx = drug_idx[idx]
#             batch_microbe_idx = microbe_idx[idx]
#             batch_labels = labels[idx]
#
#             # 4. ä»å…¨å›¾åµŒå…¥ä¸­æŠ½å–å½“å‰batchæ‰€éœ€çš„èŠ‚ç‚¹åµŒå…¥
#             drug_emb = embeddings[batch_drug_idx]
#             microbe_emb = embeddings[microbe_offset + batch_microbe_idx]
#
#             # 5. é€šè¿‡è§£ç å™¨å¾—åˆ°é¢„æµ‹ç»“æœå¹¶è®¡ç®—æŸå¤±
#             logits = decoder(drug_emb, microbe_emb)
#             loss = criterion(logits, torch.tensor(batch_labels, dtype=torch.float32, device=device))
#
#             # 6. åå‘ä¼ æ’­ï¼Œè®¡ç®—å½“å‰batchçš„æ¢¯åº¦
#             loss.backward()
#
#             # 7. æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
#             optimizer.step()
#
#             epoch_loss += loss.item()
#
#         if (epoch + 1) % 40 == 0:
#             end_time_40 = time.time()
#             elapsed_40 = end_time_40 - start_time_40
#             num_batches = num_samples // batch_size + int(num_samples % batch_size != 0)
#             avg_loss = epoch_loss / num_batches
#             print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, 40-epoch time: {elapsed_40:.2f} sec")
#
#     # è®­ç»ƒç»“æŸåï¼Œåœ¨è¯„ä¼°æ¨¡å¼ä¸‹è®¡ç®—æœ€ç»ˆçš„åµŒå…¥
#     model.eval()
#     decoder.eval()
#     with torch.no_grad():
#         final_embeddings, final_X = model(
#             (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#             edge_index, edge_weight
#         )
#
#     # åœ¨SGDæ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªbatchçš„æ¢¯åº¦ç”¨å®Œå³å¼ƒï¼Œå› æ­¤æ— æ³•åœ¨epochç»“æŸæ—¶è·å¾—æœ‰æ„ä¹‰çš„å…¨å›¾æ¢¯åº¦
#     last_epoch_gradients = None
#
#     return model, decoder, last_epoch_gradients, final_embeddings, final_X
#
# train_eval.py

# åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼Œä¸å…¶ä»– from sklearn.metrics... ä¸€èµ·æ·»åŠ  accuracy_score
from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score

# def evaluate_gcn(model, decoder, test_data, edge_index, edge_weight, drug_fg,drug_features,drug_bert, microbe_features,microbe_bert,microbe_path,microbe_offset, device='cpu',return_probs=False):
#     model.eval()
#     decoder.eval()
#     drug_idx, microbe_idx, labels = test_data
#     #drug_fg,drug_features,drug_bert= torch.tensor(drug_fg,drug_features,drug_bert,dtype=torch.float32).to(device)
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#
#     #microbe_feat = torch.tensor(microbe_features, dtype=torch.float32).to(device)
#
#
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32).to(device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32).to(device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32).to(device)
#     with torch.no_grad():
#         with torch.no_grad():
#             #adj = torch.tensor(A, dtype=torch.float32).to(device)
#
#             #embeddings ,X= model((drug_fg,drug_features,drug_bert,microbe_features,microbe_bert,microbe_path), adj)  # ç›´æ¥forward
#             embeddings, X = model((drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                                   edge_index, edge_weight)
#
#             drug_emb = embeddings[drug_idx]
#             microbe_emb = embeddings[microbe_idx + microbe_offset]
#             logits = decoder(drug_emb, microbe_emb)
#             probs = torch.sigmoid(logits).cpu().numpy()
#
#     auc = roc_auc_score(labels, probs)
#     aupr = average_precision_score(labels, probs)
#     # ã€ä¿®æ”¹2ã€‘æ ¹æ®æ–°å‚æ•°å†³å®šè¿”å›å€¼
#     if return_probs:
#         return auc, aupr, probs
#     else:
#         return auc, aupr
#
# train_eval.py

# ... (æ–‡ä»¶å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜) ...

# è¯·ç”¨è¿™ä¸ªå‡½æ•°ã€æ•´ä½“æ›¿æ¢ã€‘æ—§çš„ evaluate_gcn å‡½æ•°
def evaluate_gcn(model, decoder, data, edge_index, edge_weight,
                 drug_fg, drug_features, drug_bert,
                 microbe_features, microbe_bert, microbe_path,
                 microbe_offset, device,
                 return_probs=False):  # <--- æ ¸å¿ƒä¿®æ”¹1ï¼šå¢åŠ æ–°å‚æ•°å¹¶è®¾ç½®é»˜è®¤å€¼
    """
    è¯„ä¼°GCNæ¨¡å‹æ€§èƒ½ã€‚
    æ–°å¢åŠŸèƒ½ï¼šå¦‚æœ return_probs=Trueï¼Œåˆ™é™¤äº†è¿”å›æŒ‡æ ‡å¤–ï¼Œè¿˜è¿”å›é¢„æµ‹æ¦‚ç‡ã€‚
    """
    model.eval()
    decoder.eval()

    drug_idx, microbe_idx, labels = data

    with torch.no_grad():
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥ç‰¹å¾éƒ½æ˜¯Tensor
        # å¦‚æœå·²ç»æ˜¯Tensorï¼Œå†æ¬¡è°ƒç”¨torch.tensorä¼šåˆ›å»ºä¸€ä¸ªå‰¯æœ¬ï¼Œä½†ç±»å‹æ˜¯æ­£ç¡®çš„
        # å¦‚æœæ˜¯Numpyï¼Œåˆ™ä¼šè¿›è¡Œè½¬æ¢
        features = (
            torch.as_tensor(drug_fg, dtype=torch.float32, device=device),
            torch.as_tensor(drug_features, dtype=torch.float32, device=device),
            torch.as_tensor(drug_bert, dtype=torch.float32, device=device),
            torch.as_tensor(microbe_features, dtype=torch.float32, device=device),
            torch.as_tensor(microbe_bert, dtype=torch.float32, device=device),
            torch.as_tensor(microbe_path, dtype=torch.float32, device=device),
        )

        embeddings, _ = model(features, edge_index, edge_weight)

        x_drug = embeddings[drug_idx]
        x_microbe = embeddings[microbe_idx + microbe_offset]

        # ç¡®ä¿ x_drug å’Œ x_microbe åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        x_drug = x_drug.to(device)
        x_microbe = x_microbe.to(device)

        preds = decoder(x_drug, x_microbe).squeeze()

        # å°†é¢„æµ‹å’Œæ ‡ç­¾ç§»åˆ°CPUä¸Šè¿›è¡ŒæŒ‡æ ‡è®¡ç®—
        all_probs = torch.sigmoid(preds).cpu().numpy()
        all_labels = labels

    auc = roc_auc_score(all_labels, all_probs)
    aupr = average_precision_score(all_labels, all_probs)

    # è®¡ç®—ACC
    predicted_classes = (all_probs > 0.5).astype(int)
    acc = accuracy_score(all_labels, predicted_classes)

    # <--- æ ¸å¿ƒä¿®æ”¹2ï¼šæ ¹æ®æ–°å‚æ•°å†³å®šè¿”å›ä»€ä¹ˆ
    if return_probs:
        # å¦‚æœè°ƒç”¨è€…éœ€è¦æ¦‚ç‡å€¼ï¼Œåˆ™è¿”å› AUC, AUPR å’Œæ¦‚ç‡æ•°ç»„
        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›äº†3ä¸ªå€¼ï¼Œä¸ main.py ä¸­çš„æ¥æ”¶å˜é‡æ•°é‡ (train_auc, train_aupr, train_probs) å¯¹åº”
        return auc, aupr, all_probs
    else:
        # å¦åˆ™ï¼Œä¿æŒåŸæ¥çš„è¡Œä¸ºï¼Œè¿”å› AUC, AUPR å’Œ ACC
        return auc, aupr, acc


#å®šä¹‰ä¸€ä¸ªè®¡ç®—Fisherä¿¡æ¯çš„å‡½æ•°ã€‚Fisherä¿¡æ¯çŸ©é˜µåæ˜ äº†æ¯ä¸ªæ¨¡å‹å‚æ•°å¯¹æŸå¤±å‡½æ•°çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬å°†é€šè¿‡è®¡ç®—æ¯ä¸ªå‚æ•°çš„äºŒé˜¶æ¢¯åº¦æ¥å¾—åˆ°è¿™ä¸ªçŸ©é˜µã€‚
def compute_fisher(model, decoder, data, A, drug_features, microbe_features, microbe_offset, device):
    model.eval()
    decoder.eval()
    fisher = {name: torch.zeros_like(p) for name, p in model.named_parameters()}
    fisher_decoder = {name: torch.zeros_like(p) for name, p in decoder.named_parameters()}

    # ç”¨ä¸€éƒ¨åˆ†æ•°æ®ä¼°ç®—
    drug_idx, microbe_idx, labels = data
    drug_feat = torch.tensor(drug_features, dtype=torch.float32).to(device)
    microbe_feat = torch.tensor(microbe_features, dtype=torch.float32).to(device)
    drug_feat_reduced = model.mlp(drug_feat)
    X = build_gcn_features(drug_feat_reduced.detach().cpu().numpy(), microbe_feat.detach().cpu().numpy())
    X = torch.tensor(X, dtype=torch.float32).to(device)
    adj = torch.tensor(A, dtype=torch.float32).to(device)
    embeddings = model.gcn(X, adj)
    drug_emb = embeddings[drug_idx]
    microbe_emb = embeddings[microbe_offset + microbe_idx]
    logits = decoder(drug_emb, microbe_emb)
    loss = nn.BCEWithLogitsLoss()(logits, torch.tensor(labels, dtype=torch.float32).to(device))

    # è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å¹³æ–¹ï¼ˆè¿‘ä¼¼Fisherï¼‰
    loss.backward()
    for name, p in model.named_parameters():
        if p.grad is not None:
            fisher[name] += (p.grad.detach() ** 2)
    for name, p in decoder.named_parameters():
        if p.grad is not None:
            fisher_decoder[name] += (p.grad.detach() ** 2)
    return fisher, fisher_decoder

#æ–°å¢ EWC æŸå¤±å‡½æ•°
# def ewc_loss_fn(model, decoder, old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc):
#     ewc_loss = 0
#     for name, param in model.named_parameters():
#         if name in old_params and name in fisher:
#             if param.shape == old_params[name].shape and param.shape == fisher[name].shape:
#                 ewc_loss += (fisher[name] * (param - old_params[name]) ** 2).sum()
#             #else:
#                 print(f"[EWC][SKIP] name: {name} shape: {param.shape}, old: {old_params[name].shape}, fisher: {fisher[name].shape}")
#         else:
#             print(f"[EWC][SKIP] name: {name} not found in old_params/fisher")
#     # decoderåŒç†
#     for name, param in decoder.named_parameters():
#         if name in old_params_decoder and name in fisher_decoder:
#             if param.shape == old_params_decoder[name].shape and param.shape == fisher_decoder[name].shape:
#                 ewc_loss += (fisher_decoder[name] * (param - old_params_decoder[name]) ** 2).sum()
#             #else:
#                 print(f"[EWC][DECODER][SKIP] name: {name} shape: {param.shape}, old: {old_params_decoder[name].shape}, fisher: {fisher_decoder[name].shape}")
#         #else:
#             print(f"[EWC][DECODER][SKIP] name: {name} not found in old_params_decoder/fisher_decoder")
#     return lambda_ewc * ewc_loss
def ewc_loss_fn(model, decoder, old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc, print_once_set=None):
    ewc_loss = 0.0
    if print_once_set is None:
        print_once_set = set()
    # åªå¯¹gcn.ç›¸å…³å‚æ•°
    for name, param in model.named_parameters():
        if 'gcn.conv2' in name:
            if name in old_params and name in fisher:
                if param.shape == old_params[name].shape and param.shape == fisher[name].shape:
                    ewc_loss += (fisher[name] * (param - old_params[name]) ** 2).sum()
                    if name not in print_once_set:
                        #print(f"[EWC][APPLY] model: {name} shape: {param.shape}")
                        print_once_set.add(name)
    for name, param in decoder.named_parameters():
        if name in old_params_decoder and name in fisher_decoder:
            if param.shape == old_params_decoder[name].shape and param.shape == fisher_decoder[name].shape:
                ewc_loss += (fisher_decoder[name] * (param - old_params_decoder[name]) ** 2).sum()
                if name not in print_once_set:
                    #print(f"[EWC][APPLY] decoder: {name} shape: {param.shape}")
                    print_once_set.add(name)
    return lambda_ewc * ewc_loss





def compute_fisher_gcn(model, decoder, data, edge_index, edge_weight, drug_fg, drug_features, drug_bert,
                       microbe_features, microbe_bert, microbe_path, microbe_offset, device):
    """
    è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µï¼ˆä¸GCNå…¼å®¹çš„ç‰ˆæœ¬ï¼‰
    """
    model.eval()
    decoder.eval()
    fisher = {name: torch.zeros_like(p) for name, p in model.named_parameters() if p.requires_grad}
    fisher_decoder = {name: torch.zeros_like(p) for name, p in decoder.named_parameters() if p.requires_grad}

    drug_idx, microbe_idx, labels = data

    # è½¬æ¢ä¸ºtensor
    drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
    drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
    drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
    microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
    microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
    microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)

    # å‰å‘ä¼ æ’­
    embeddings, X = model((drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
                          edge_index, edge_weight)
    drug_emb = embeddings[drug_idx]
    microbe_emb = embeddings[microbe_offset + microbe_idx]
    logits = decoder(drug_emb, microbe_emb)
    loss = nn.BCEWithLogitsLoss()(logits, torch.tensor(labels, dtype=torch.float32, device=device))

    # è®¡ç®—æ¢¯åº¦
    model.zero_grad()
    decoder.zero_grad()
    loss.backward()

    num_samples = len(drug_idx)  # â˜… åŠ è¿™ä¸€è¡Œ
    for name, p in model.named_parameters():
        if p.requires_grad and p.grad is not None:
            fisher[name] += (p.grad.detach() ** 2) / num_samples  # â˜… è¿™é‡Œé™¤ä»¥æ ·æœ¬æ•°
    for name, p in decoder.named_parameters():
        if p.requires_grad and p.grad is not None:
            fisher_decoder[name] += (p.grad.detach() ** 2) / num_samples  # â˜… åŒä¸Š
    # # ä¿å­˜æ¢¯åº¦å¹³æ–¹ä½œä¸ºFisherä¿¡æ¯è¿‘ä¼¼
    # for name, p in model.named_parameters():
    #     if p.requires_grad and p.grad is not None:
    #         fisher[name] += (p.grad.detach() ** 2)
    # for name, p in decoder.named_parameters():
    #     if p.requires_grad and p.grad is not None:
    #         fisher_decoder[name] += (p.grad.detach() ** 2)



    return fisher, fisher_decoder



#
# def train_gcn_ewc_new(
#         train_data, edge_index, edge_weight, drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#         epochs, lr, hidden, dropout, device,
#         old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc,
#         model=None, decoder=None, args=None
# ):
#     """
#     æ”¯æŒEWCçš„GCNè®­ç»ƒå‡½æ•°ï¼ˆæ–°ç‰ˆæœ¬ï¼Œä¸ä¸»ç¨‹åºå…¼å®¹ï¼‰
#     """
#     model = model.to(device)
#     decoder = decoder.to(device)
#
#     # åªä¼˜åŒ–éœ€è¦è®­ç»ƒçš„å‚æ•°
#     optimizer = torch.optim.Adam(
#         filter(lambda p: p.requires_grad, list(model.parameters()) + list(decoder.parameters())),
#         lr=lr
#     )
#     criterion = nn.BCEWithLogitsLoss()
#     model.train()
#     decoder.train()
#
#     # è½¬æ¢ä¸ºtensorï¼ˆä¸€æ¬¡æ€§è½¬æ¢ï¼‰
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     print_once_set = set()  # ç”¨äºæ§åˆ¶EWCæ—¥å¿—åªæ‰“å°ä¸€æ¬¡
#
#     for epoch in range(epochs):
#         optimizer.zero_grad()
#         drug_idx, microbe_idx, labels = train_data
#
#         # è·å–GCNåµŒå…¥
#         embeddings, X = model((drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#                               edge_index, edge_weight)
#
#         drug_emb = embeddings[drug_idx]
#         microbe_emb = embeddings[microbe_offset + microbe_idx]
#         logits = decoder(drug_emb, microbe_emb)
#
#         # ä¸»ä»»åŠ¡æŸå¤±
#         main_loss = criterion(logits, torch.tensor(labels, dtype=torch.float32, device=device))
#
#         # EWCæŸå¤±
#         ewc_loss = ewc_loss_fn(model, decoder, old_params, old_params_decoder,
#                                fisher, fisher_decoder, lambda_ewc, print_once_set)
#
#         total_loss = main_loss + ewc_loss
#         total_loss.backward()
#         optimizer.step()
#
#         if (epoch + 1) % 40 == 0:
#             print(
#                 f"[EWC] Epoch {epoch + 1}/{epochs}, Main Loss: {main_loss.item():.4f}, EWC Loss: {ewc_loss.item():.4f}, Total: {total_loss.item():.4f}")
#
#     return model, decoder
# =================================================================================
#  è¯·åœ¨ train_eval.py æ–‡ä»¶ä¸­ï¼Œç”¨ä»¥ä¸‹å®Œæ•´ä»£ç æ›¿æ¢æ—§çš„ train_gcn_ewc_new å‡½æ•°
# =================================================================================

# def train_gcn_ewc_new(
#         # --- DrugVirus (æ–°ä»»åŠ¡) æ•°æ® ---
#         train_data, edge_index, edge_weight, drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#
#         # --- MDAD (æ—§ä»»åŠ¡) æ•°æ® (æ–°å¢å‚æ•°) ---
#         mdad_train_data, mdad_edge_index, mdad_edge_weight,
#         mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
#         mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
#         mdad_microbe_offset,
#
#         # --- è®­ç»ƒè¶…å‚æ•° ---
#         epochs, lr, hidden, dropout, device,
#
#         # --- EWC ç›¸å…³å‚æ•° ---
#         old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc,
#
#         # --- å…¶ä»– ---
#         model=None, decoder=None, args=None,
#
#         # --- æ–°å¢ï¼šä»»åŠ¡æƒé‡è¶…å‚æ•° ---
#         alpha=0.5,
#     # --- æ–°å¢ï¼šæƒé‡è¡°å‡ ---
#         weight_decay = 1e-5  # <--- ã€æ ¸å¿ƒä¿®æ”¹1ï¼šå¢åŠ å‚æ•°ã€‘
# ):
#     """
#     æ”¯æŒEWCå’Œå¤šä»»åŠ¡æ’ç»ƒçš„GCNè®­ç»ƒå‡½æ•°ã€‚
#     """
#     model = model.to(device)
#     decoder = decoder.to(device)
#
#     optimizer = torch.optim.Adam(
#         filter(lambda p: p.requires_grad, list(model.parameters()) + list(decoder.parameters())),
#         lr=lr,
#         weight_decay=weight_decay
#     )
#     # ======================== ã€åœ¨è¿™é‡Œæ–°å¢ã€‘ ========================
#     # 1. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
#     scheduler = torch.optim.lr_scheduler.StepLR(
#         optimizer,
#         step_size=args.lr_step_size,
#         gamma=args.lr_gamma
#     )
#     # ===============================================================
#
#     criterion = nn.BCEWithLogitsLoss()
#     model.train()
#     decoder.train()
#
#     # --- ä¸€æ¬¡æ€§è½¬æ¢ DrugVirus ç‰¹å¾ä¸º Tensor ---
#     drug_fg = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     # --- ä¸€æ¬¡æ€§è½¬æ¢ MDAD ç‰¹å¾ä¸º Tensor (æ–°å¢) ---
#     mdad_drug_fg_norm = torch.tensor(mdad_drug_fg_norm, dtype=torch.float32, device=device)
#     mdad_drug_features_norm = torch.tensor(mdad_drug_features_norm, dtype=torch.float32, device=device)
#     mdad_drug_bert_norm = torch.tensor(mdad_drug_bert_norm, dtype=torch.float32, device=device)
#     mdad_microbe_features_norm = torch.tensor(mdad_microbe_features_norm, dtype=torch.float32, device=device)
#     mdad_microbe_bert_norm = torch.tensor(mdad_microbe_bert_norm, dtype=torch.float32, device=device)
#     mdad_microbe_path_norm = torch.tensor(mdad_microbe_path_norm, dtype=torch.float32, device=device)
#
#     print_once_set = set()
#
#     for epoch in range(epochs):
#         optimizer.zero_grad()
#
#         # ==================== æ ¸å¿ƒä¿®æ”¹åŒºåŸŸå¼€å§‹ ====================
#
#         # --- 1. DrugVirus (æ–°ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— ---
#         drug_idx, microbe_idx, labels = train_data
#         embeddings_dv, _ = model(
#             (drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path),
#             edge_index, edge_weight
#         )
#         drug_emb_dv = embeddings_dv[drug_idx]
#         microbe_emb_dv = embeddings_dv[microbe_offset + microbe_idx]
#         logits_dv = decoder(drug_emb_dv, microbe_emb_dv)
#         loss_drugvirus = criterion(logits_dv, torch.tensor(labels, dtype=torch.float32, device=device))
#
#         # --- 2. MDAD (æ—§ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— (æ–°å¢) ---
#         mdad_drug_idx, mdad_microbe_idx, mdad_labels = mdad_train_data
#         embeddings_mdad, _ = model(
#             (mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
#              mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm),
#             mdad_edge_index, mdad_edge_weight
#         )
#         drug_emb_mdad = embeddings_mdad[mdad_drug_idx]
#         microbe_emb_mdad = embeddings_mdad[mdad_microbe_offset + mdad_microbe_idx]
#         logits_mdad = decoder(drug_emb_mdad, microbe_emb_mdad)
#         loss_mdad = criterion(logits_mdad, torch.tensor(mdad_labels, dtype=torch.float32, device=device))
#
#         # --- 3. EWC æŸå¤±è®¡ç®— (ä¸å˜) ---
#         ewc_loss = ewc_loss_fn(model, decoder, old_params, old_params_decoder,
#                                fisher, fisher_decoder, lambda_ewc, print_once_set)
#
#         # --- 4. æ„å»ºæœ€ç»ˆçš„æ€»æŸå¤± (ä¿®æ”¹) ---
#         # æ€»æŸå¤± = (1-alpha)*æ–°ä»»åŠ¡æŸå¤± + alpha*æ—§ä»»åŠ¡æŸå¤± + EWCæƒ©ç½š
#         total_loss = (1 - alpha) * loss_drugvirus + alpha * loss_mdad + ewc_loss
#
#         # ==================== æ ¸å¿ƒä¿®æ”¹åŒºåŸŸç»“æŸ ====================
#
#         total_loss.backward()
#         optimizer.step()
#         # ======================== ã€åœ¨è¿™é‡Œæ–°å¢ã€‘ ========================
#         # 2. åœ¨æ¯ä¸ªepochç»“æŸåï¼Œæ›´æ–°å­¦ä¹ ç‡
#         scheduler.step()
#         # ===============================================================
#
#         if (epoch + 1) % 40 == 0:
#             print(
#                 f"[EWC-MTL] Epoch {epoch + 1}/{epochs}, "
#                 f"Loss_DV: {loss_drugvirus.item():.4f}, "
#                 f"Loss_MDAD: {loss_mdad.item():.4f}, "
#                 f"Loss_EWC: {ewc_loss.item():.4f}, "
#                 f"Total: {total_loss.item():.4f}"
#             )
#
#     return model, decoder

# æ›¿æ¢ train_eval.py ä¸­æ—§çš„ train_gcn_ewc_new å‡½æ•°
# def train_gcn_ewc_new(
#         # --- DrugVirus (æ–°ä»»åŠ¡) æ•°æ® ---
#         train_data, edge_index, edge_weight, drug_fg, drug_features, drug_bert,
#         microbe_features, microbe_bert, microbe_path, microbe_offset,
#
#         # --- MDAD (æ—§ä»»åŠ¡) æ•°æ® (æ–°å¢å‚æ•°) ---
#         mdad_test_data,mdad_train_data, mdad_edge_index, mdad_edge_weight,
#         mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
#         mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
#         mdad_microbe_offset,
#
#
#
#
#
#         # --- è®­ç»ƒè¶…å‚æ•° ---
#         epochs, lr, hidden, dropout, device,
#
#         # --- EWC ç›¸å…³å‚æ•° ---
#         old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc,
#
#         # --- å…¶ä»– ---
#         model=None, decoder=None, args=None,
#
#
#
#         # --- æ–°å¢ï¼šä»»åŠ¡æƒé‡è¶…å‚æ•° ---
#         alpha=0.5,
#         weight_decay=1e-5,
#
#         # ==================== ã€æ ¸å¿ƒä¿®æ”¹1ï¼šå¢åŠ ç»˜å›¾ç›¸å…³å‚æ•°ã€‘ ====================
#         drugvirus_test_data=None,
#         fold_num=0,
#         save_dir='.'
#         # ====================================================================
# ):
#     """
#     æ”¯æŒEWCå’Œå¤šä»»åŠ¡æ’ç»ƒçš„GCNè®­ç»ƒå‡½æ•°ã€‚
#     """
#     model = model.to(device)
#     decoder = decoder.to(device)
#
#     optimizer = torch.optim.Adam(
#         filter(lambda p: p.requires_grad, list(model.parameters()) + list(decoder.parameters())),
#         lr=lr,
#         weight_decay=weight_decay
#     )
#     scheduler = torch.optim.lr_scheduler.StepLR(
#         optimizer,
#         step_size=args.lr_step_size,
#         gamma=args.lr_gamma
#     )
#
#     criterion = nn.BCEWithLogitsLoss()
#     model.train()
#     decoder.train()
#
#     # --- ä¸€æ¬¡æ€§è½¬æ¢ DrugVirus ç‰¹å¾ä¸º Tensor ---
#     drug_fg_t = torch.tensor(drug_fg, dtype=torch.float32, device=device)
#     drug_features_t = torch.tensor(drug_features, dtype=torch.float32, device=device)
#     drug_bert_t = torch.tensor(drug_bert, dtype=torch.float32, device=device)
#     microbe_features_t = torch.tensor(microbe_features, dtype=torch.float32, device=device)
#     microbe_bert_t = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
#     microbe_path_t = torch.tensor(microbe_path, dtype=torch.float32, device=device)
#
#     # --- ä¸€æ¬¡æ€§è½¬æ¢ MDAD ç‰¹å¾ä¸º Tensor ---
#     mdad_drug_fg_norm_t = torch.tensor(mdad_drug_fg_norm, dtype=torch.float32, device=device)
#     mdad_drug_features_norm_t = torch.tensor(mdad_drug_features_norm, dtype=torch.float32, device=device)
#     mdad_drug_bert_norm_t = torch.tensor(mdad_drug_bert_norm, dtype=torch.float32, device=device)
#     mdad_microbe_features_norm_t = torch.tensor(mdad_microbe_features_norm, dtype=torch.float32, device=device)
#     mdad_microbe_bert_norm_t = torch.tensor(mdad_microbe_bert_norm, dtype=torch.float32, device=device)
#     mdad_microbe_path_norm_t = torch.tensor(mdad_microbe_path_norm, dtype=torch.float32, device=device)
#
#     print_once_set = set()
#
#     # ==================== ã€æ ¸å¿ƒä¿®æ”¹2ï¼šåˆå§‹åŒ–ç»˜å›¾åˆ—è¡¨ã€‘ ====================
#     plot_epochs = []
#     plot_aucs = []
#     plot_auprs = []
#     # ====================================================================
#     mdad_plot_epochs = []
#     mdad_plot_aucs = []
#     mdad_plot_auprs = []
#
#
#     for epoch in range(epochs):
#         optimizer.zero_grad()
#
#         # --- 1. DrugVirus (æ–°ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— ---
#         drug_idx, microbe_idx, labels = train_data
#         embeddings_dv, _ = model(
#             (drug_fg_t, drug_features_t, drug_bert_t, microbe_features_t, microbe_bert_t, microbe_path_t),
#             edge_index, edge_weight
#         )
#         drug_emb_dv = embeddings_dv[drug_idx]
#         microbe_emb_dv = embeddings_dv[microbe_offset + microbe_idx]
#         logits_dv = decoder(drug_emb_dv, microbe_emb_dv)
#         loss_drugvirus = criterion(logits_dv, torch.tensor(labels, dtype=torch.float32, device=device))
#
#         # --- 2. MDAD (æ—§ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— ---
#         mdad_drug_idx, mdad_microbe_idx, mdad_labels = mdad_train_data
#         embeddings_mdad, _ = model(
#             (mdad_drug_fg_norm_t, mdad_drug_features_norm_t, mdad_drug_bert_norm_t,
#              mdad_microbe_features_norm_t, mdad_microbe_bert_norm_t, mdad_microbe_path_norm_t),
#             mdad_edge_index, mdad_edge_weight
#         )
#         drug_emb_mdad = embeddings_mdad[mdad_drug_idx]
#         microbe_emb_mdad = embeddings_mdad[mdad_microbe_offset + mdad_microbe_idx]
#         logits_mdad = decoder(drug_emb_mdad, microbe_emb_mdad)
#         loss_mdad = criterion(logits_mdad, torch.tensor(mdad_labels, dtype=torch.float32, device=device))
#
#         # --- 3. EWC æŸå¤±è®¡ç®— ---
#         ewc_loss = ewc_loss_fn(model, decoder, old_params, old_params_decoder,
#                                fisher, fisher_decoder, lambda_ewc, print_once_set)
#
#         # --- 4. æ„å»ºæœ€ç»ˆçš„æ€»æŸå¤± ---
#         total_loss = (1 - alpha) * loss_drugvirus + alpha * loss_mdad + ewc_loss
#
#         total_loss.backward()
#         optimizer.step()
#         scheduler.step()
#
#         # ==================== ã€æ ¸å¿ƒä¿®æ”¹3ï¼šå®šæœŸè¯„ä¼°å¹¶è®°å½•æ•°æ®ã€‘ ====================
#         if (epoch + 1) % 6 == 0 and drugvirus_test_data is not None:
#             model.eval()
#             decoder.eval()
#             with torch.no_grad():
#                 # æ³¨æ„ï¼šè¿™é‡Œè¯„ä¼°çš„æ˜¯ DrugVirus çš„æ€§èƒ½
#                 test_auc, test_aupr = evaluate_gcn(
#                     model, decoder, drugvirus_test_data, edge_index, edge_weight,
#                     drug_fg, drug_features, drug_bert,
#                     microbe_features, microbe_bert, microbe_path,
#                     microbe_offset, device
#                 )
#             plot_epochs.append(epoch + 1)
#             plot_aucs.append(test_auc)
#             plot_auprs.append(test_aupr)
#
#             mdad_auc, mdad_aupr = evaluate_gcn(
#                 model, decoder, mdad_test_data, mdad_edge_index, mdad_edge_weight,
#                 mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
#                 mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
#                 mdad_microbe_offset, device
#             )
#             mdad_plot_epochs.append(epoch + 1)
#             mdad_plot_aucs.append(mdad_auc)
#             mdad_plot_auprs.append(mdad_aupr)
#
#
#             model.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
#             decoder.train()
#         # ========================================================================
#
#         if (epoch + 1) % 40 == 0:
#             print(
#                 f"[EWC-MTL Fold {fold_num + 1}] Epoch {epoch + 1}/{epochs}, "
#                 f"Loss_DV: {loss_drugvirus.item():.4f}, "
#                 f"Loss_MDAD: {loss_mdad.item():.4f}, "
#                 f"Loss_EWC: {ewc_loss.item():.4f}, "
#                 f"Total: {total_loss.item():.4f}"
#             )
#
#
#     # ==================== ã€æ ¸å¿ƒä¿®æ”¹4ï¼šè®­ç»ƒç»“æŸåç»˜å›¾å¹¶ä¿å­˜ã€‘ ====================
#     if plot_epochs:
#         plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
#         plt.rcParams['axes.unicode_minus'] = False
#
#         plt.figure(figsize=(12, 8))
#         plt.plot(plot_epochs, plot_aucs, marker='o', linestyle='-', label='DrugVirus Test AUC')
#         plt.plot(plot_epochs, plot_auprs, marker='s', linestyle='--', label='DrugVirus Test AUPR')
#         plt.title(f'Incremental Learning Curve (Fold {fold_num + 1})', fontsize=16)
#         plt.xlabel('Epoch', fontsize=12)
#         plt.ylabel('Score', fontsize=12)
#         plt.legend(fontsize=12)
#         plt.grid(True)
#
#         os.makedirs(save_dir, exist_ok=True)
#         # ä½¿ç”¨å›ºå®šçš„æ–‡ä»¶åæ¥ç¡®ä¿è¦†ç›–
#         plot_filename = f'incremental_learning_fold_{fold_num + 1}.png'
#         save_path = os.path.join(save_dir, plot_filename)
#         plt.savefig(save_path, dpi=300)
#         plt.close()
#         print(f"æˆåŠŸ: å¢é‡å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜ -> {save_path}")
#     # ========================================================================
#
#     if mdad_plot_epochs:
#         plt.figure(figsize=(12, 8))
#         plt.plot(mdad_plot_epochs, mdad_plot_aucs, marker='o', label='MDAD AUC')
#         plt.plot(mdad_plot_epochs, mdad_plot_auprs, marker='s', label='MDAD AUPR')
#         plt.title(f'MDAD Performance vs Epoch (Fold {fold_num + 1})', fontsize=16)
#         plt.xlabel('Epoch')
#         plt.ylabel('Score')
#         plt.legend()
#         plt.grid(True)
#         os.makedirs(save_dir, exist_ok=True)
#         mdad_path = os.path.join(save_dir, f'mdad_epoch_curve_fold_{fold_num + 1}.png')
#         plt.savefig(mdad_path, dpi=300)
#         plt.close()
#         print(f"MDADæ—§ä»»åŠ¡æ›²çº¿å·²ä¿å­˜è‡³: {mdad_path}")
#
#
#     return model, decoder



# æ›¿æ¢ train_eval.py ä¸­æ—§çš„ train_gcn_ewc_new å‡½æ•°
def train_gcn_ewc_new(
        # --- DrugVirus (æ–°ä»»åŠ¡) æ•°æ® ---
        train_data, edge_index, edge_weight, drug_fg, drug_features, drug_bert,
        microbe_features, microbe_bert, microbe_path, microbe_offset,

        # --- MDAD (æ—§ä»»åŠ¡) æ•°æ® ---
        mdad_test_data, mdad_train_data, mdad_edge_index, mdad_edge_weight,
        mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
        mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
        mdad_microbe_offset,

        # --- è®­ç»ƒè¶…å‚æ•° ---
        epochs, lr, hidden, dropout, device,

        # --- EWC ç›¸å…³å‚æ•° ---
        old_params, old_params_decoder, fisher, fisher_decoder, lambda_ewc,

        # --- å…¶ä»– ---
        model=None, decoder=None, args=None,

        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: æ¥æ”¶å¯¹é½MLPæ¨¡å— ---
        alignment_mlps=None,

        # ========== ã€åœ¨è¿™é‡Œæ–°å¢ã€‘æ¥æ”¶ç‰¹å¾å¯¹é½å¼€å…³ ==========
        use_feature_alignment=True,

        # --- ä»»åŠ¡æƒé‡è¶…å‚æ•° ---
        alpha=0.5,
        weight_decay=1e-5,

        # --- ç»˜å›¾ç›¸å…³å‚æ•° ---
        drugvirus_test_data=None,
        fold_num=0,
        save_dir='.'


):

    """
    ã€ä¿®æ”¹ç‰ˆã€‘æ”¯æŒEWCã€å¤šä»»åŠ¡æ’ç»ƒï¼Œå¹¶ä¸ç‰¹å¾å¯¹é½MLPè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„GCNå‡½æ•°ã€‚
    """
    model = model.to(device)
    decoder = decoder.to(device)



    # ========================= ã€åœ¨æ­¤å¤„æ·»åŠ ã€‘ =========================
    # å°†ä¼ å…¥çš„å†…éƒ¨å¯¹é½MLPé™„åŠ åˆ°ä¸»æ¨¡å‹å¯¹è±¡ä¸Šï¼Œæ–¹ä¾¿åç»­åœ¨å¤–éƒ¨è°ƒç”¨
    if alignment_mlps is not None:
        model.alignment_mlps = alignment_mlps.to(device)
    else:
        model.alignment_mlps = None
    # =================================================================




    # # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: å°†å¯¹é½MLPçš„å‚æ•°åŠ å…¥ä¼˜åŒ–å™¨ ---
    # params_to_optimize = list(filter(lambda p: p.requires_grad, model.parameters())) + \
    #                      list(filter(lambda p: p.requires_grad, decoder.parameters()))
    # if alignment_mlps is not None:
    #     alignment_mlps.train()  # ç¡®ä¿å®ƒåœ¨è®­ç»ƒæ¨¡å¼
    #     params_to_optimize += list(alignment_mlps.parameters())
    #     print("ä¿¡æ¯: å¯¹é½MLPçš„å‚æ•°å·²åŠ å…¥ä¼˜åŒ–å™¨ï¼Œå°†è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚")
    # else:
    #     print("è­¦å‘Š: æœªæä¾›å¯¹é½MLP (alignment_mlps=None)ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹DrugVirusç‰¹å¾ã€‚")
    # train_eval.py, ~1000è¡Œ (æ›¿æ¢åçš„ä»£ç )

    # train_eval.py, ~1000è¡Œ (æ›¿æ¢æˆè¿™ä¸ªä¿®æ­£ç‰ˆ)

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: ä¸ºä¸åŒæ¨¡å—è®¾ç½®ç‹¬ç«‹çš„å­¦ä¹ ç‡ (å·²ä¿®å¤å‚æ•°é‡å¤é—®é¢˜) ---
    # 1. å®šä¹‰ä¸€ä¸ªæ–°çš„å­¦ä¹ ç‡å‚æ•°ï¼Œä¸“é—¨ç»™å¯¹é½MLPç”¨
    lr_align = 0.00001  # <--- åœ¨è¿™é‡Œè®¾ç½®å¯¹é½MLPçš„ä¸“å±å­¦ä¹ ç‡.0.00001

    # 2. å‡†å¤‡ä¸€ä¸ªåˆ—è¡¨æ¥å­˜æ”¾æ‰€æœ‰çš„å‚æ•°ç»„
    params_groups = []
    main_model_params = []  # ç”¨æ¥å­˜æ”¾ä¸åŒ…å«å¯¹é½MLPçš„ä¸»æ¨¡å‹å‚æ•°

    if alignment_mlps is not None and use_feature_alignment:
        # å¦‚æœå¯ç”¨å¯¹é½ï¼Œæˆ‘ä»¬éœ€è¦å°†ä¸»æ¨¡å‹çš„å‚æ•°å’Œå¯¹é½MLPçš„å‚æ•°åˆ†å¼€
        alignment_mlps.train()

        # è·å–å¯¹é½MLPå‚æ•°çš„IDï¼Œç”¨äºåç»­è¿‡æ»¤
        align_param_ids = set(id(p) for p in alignment_mlps.parameters())

        # è¿‡æ»¤å‡ºä¸å±äºå¯¹é½MLPçš„ä¸»æ¨¡å‹å‚æ•°
        main_model_params = [p for p in model.parameters() if id(p) not in align_param_ids and p.requires_grad]

        # ä¸ºå¯¹é½MLPåˆ›å»ºç‹¬ç«‹çš„å‚æ•°ç»„
        params_groups.append({
            'params': alignment_mlps.parameters(),
            'lr': lr_align  # <--- ä½¿ç”¨ä¸“å±å­¦ä¹ ç‡
        })
        print(f"ä¿¡æ¯: ç‰¹å¾å¯¹é½å·²å¯ç”¨ï¼Œä¸»æ¨¡å‹LR={lr}, å¯¹é½MLP LR={lr_align}ã€‚")
    else:
        # å¦‚æœä¸å¯ç”¨å¯¹é½ï¼Œæ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½ä½¿ç”¨ä¸»å­¦ä¹ ç‡
        main_model_params = list(filter(lambda p: p.requires_grad, model.parameters()))
        if alignment_mlps is not None and not use_feature_alignment:
            print("ä¿¡æ¯: ç‰¹å¾å¯¹é½å·²ç¦ç”¨ï¼Œå°†ä¸è®­ç»ƒå¯¹é½MLPå¹¶ä½¿ç”¨åŸå§‹DrugVirusç‰¹å¾ã€‚")
        else:
            print("è­¦å‘Š: æœªæä¾›å¯¹é½MLP (alignment_mlps=None)ï¼Œå°†ç›´æ¥ä½¿ç”¨åŸå§‹DrugVirusç‰¹å¾ã€‚")

    # 3. å°†ä¸»æ¨¡å‹ï¼ˆå·²è¿‡æ»¤ï¼‰å’Œè§£ç å™¨çš„å‚æ•°åŠ å…¥å‚æ•°ç»„åˆ—è¡¨
    params_groups.append({
        'params': main_model_params,
        'lr': lr  # ä½¿ç”¨ä¸»å­¦ä¹ ç‡
    })
    params_groups.append({
        'params': filter(lambda p: p.requires_grad, decoder.parameters()),
        'lr': lr  # ä½¿ç”¨ä¸»å­¦ä¹ ç‡
    })

    # 4. å°†å‚æ•°ç»„åˆ—è¡¨ä¼ ç»™ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        params_groups,  # <--- ä¼ å…¥ä¿®å¤åçš„å‚æ•°ç»„åˆ—è¡¨
        weight_decay=weight_decay
    )

    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=args.lr_step_size,
        gamma=args.lr_gamma
    )

    criterion = nn.BCEWithLogitsLoss()
    model.train()
    decoder.train()

    # --- ä¸€æ¬¡æ€§è½¬æ¢ MDAD ç‰¹å¾ä¸º Tensor (ä¿æŒä¸å˜) ---
    mdad_drug_fg_norm_t = torch.tensor(mdad_drug_fg_norm, dtype=torch.float32, device=device)
    mdad_drug_features_norm_t = torch.tensor(mdad_drug_features_norm, dtype=torch.float32, device=device)
    mdad_drug_bert_norm_t = torch.tensor(mdad_drug_bert_norm, dtype=torch.float32, device=device)
    mdad_microbe_features_norm_t = torch.tensor(mdad_microbe_features_norm, dtype=torch.float32, device=device)
    mdad_microbe_bert_norm_t = torch.tensor(mdad_microbe_bert_norm, dtype=torch.float32, device=device)
    mdad_microbe_path_norm_t = torch.tensor(mdad_microbe_path_norm, dtype=torch.float32, device=device)

    print_once_set = set()

    # åˆå§‹åŒ–ç»˜å›¾åˆ—è¡¨
    plot_epochs, plot_aucs, plot_auprs = [], [], []
    mdad_plot_epochs, mdad_plot_aucs, mdad_plot_auprs = [], [], []

    for epoch in range(epochs):
        optimizer.zero_grad()

        # --- 1. DrugVirus (æ–°ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— ---
        drug_idx, microbe_idx, labels = train_data

        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: åœ¨å¾ªç¯å†…è¿›è¡Œç‰¹å¾å¯¹é½ ---
        # å°†åŸå§‹DrugVirusç‰¹å¾è½¬ä¸ºTensor
        drug_fg_raw_t = torch.tensor(drug_fg, dtype=torch.float32, device=device)
        drug_features_raw_t = torch.tensor(drug_features, dtype=torch.float32, device=device)
        drug_bert_raw_t = torch.tensor(drug_bert, dtype=torch.float32, device=device)
        microbe_features_raw_t = torch.tensor(microbe_features, dtype=torch.float32, device=device)
        microbe_bert_raw_t = torch.tensor(microbe_bert, dtype=torch.float32, device=device)
        microbe_path_raw_t = torch.tensor(microbe_path, dtype=torch.float32, device=device)

        drugvirus_raw_feats = [
            drug_fg_raw_t, drug_features_raw_t, drug_bert_raw_t,
            microbe_features_raw_t, microbe_bert_raw_t, microbe_path_raw_t
        ]

        # å¦‚æœæœ‰å¯¹é½MLPï¼Œåˆ™åº”ç”¨å˜æ¢ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
        # if alignment_mlps is not None:
        #     aligned_feats = [
        #         alignment_mlps[i](feat) for i, feat in enumerate(drugvirus_raw_feats)
        #     ]
        # else:
        #     aligned_feats = drugvirus_raw_feats
        # å¦‚æœæœ‰å¯¹é½MLPä¸”å¼€å…³ä¸ºTrueï¼Œåˆ™åº”ç”¨å˜æ¢ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
        if alignment_mlps is not None and use_feature_alignment:
            aligned_feats = [
                alignment_mlps[i](feat) for i, feat in enumerate(drugvirus_raw_feats)
            ]
        else:
            aligned_feats = drugvirus_raw_feats

        # å°†å¯¹é½åçš„ç‰¹å¾ä¼ é€’ç»™æ¨¡å‹
        embeddings_dv, _ = model(
            tuple(aligned_feats),  # ä¼ å…¥å¯¹é½åçš„ç‰¹å¾å…ƒç»„
            edge_index, edge_weight
        )
        drug_emb_dv = embeddings_dv[drug_idx]
        microbe_emb_dv = embeddings_dv[microbe_offset + microbe_idx]
        logits_dv = decoder(drug_emb_dv, microbe_emb_dv)
        loss_drugvirus = criterion(logits_dv, torch.tensor(labels, dtype=torch.float32, device=device))

        # --- 2. MDAD (æ—§ä»»åŠ¡) å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®— (ä¿æŒä¸å˜) ---
        mdad_drug_idx, mdad_microbe_idx, mdad_labels = mdad_train_data
        embeddings_mdad, _ = model(
            (mdad_drug_fg_norm_t, mdad_drug_features_norm_t, mdad_drug_bert_norm_t,
             mdad_microbe_features_norm_t, mdad_microbe_bert_norm_t, mdad_microbe_path_norm_t),
            mdad_edge_index, mdad_edge_weight
        )
        drug_emb_mdad = embeddings_mdad[mdad_drug_idx]
        microbe_emb_mdad = embeddings_mdad[mdad_microbe_offset + mdad_microbe_idx]
        logits_mdad = decoder(drug_emb_mdad, microbe_emb_mdad)
        loss_mdad = criterion(logits_mdad, torch.tensor(mdad_labels, dtype=torch.float32, device=device))

        # --- 3. EWC æŸå¤±è®¡ç®— (ä¿æŒä¸å˜) ---
        ewc_loss = ewc_loss_fn(model, decoder, old_params, old_params_decoder,
                               fisher, fisher_decoder, lambda_ewc, print_once_set)

        # --- 4. æ„å»ºæœ€ç»ˆçš„æ€»æŸå¤± (ä¿æŒä¸å˜) ---
        total_loss = (1 - alpha) * loss_drugvirus + alpha * loss_mdad + ewc_loss

        total_loss.backward()
        optimizer.step()
        scheduler.step()

        # --- å®šæœŸè¯„ä¼°å¹¶è®°å½•æ•°æ® ---
        if (epoch + 1) % 6 == 0 and drugvirus_test_data is not None:
            model.eval()
            decoder.eval()
            if alignment_mlps: alignment_mlps.eval()

            with torch.no_grad():
                # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘: è¯„ä¼°å‰ä¹Ÿéœ€è¦å¯¹é½ç‰¹å¾ ---
                if alignment_mlps is not None:
                    aligned_feats_eval = [mlp(feat).cpu().numpy() for mlp, feat in zip(alignment_mlps, drugvirus_raw_feats)]
                    drug_fg_eval, drug_features_eval, drug_bert_eval, \
                    microbe_features_eval, microbe_bert_eval, microbe_path_eval = aligned_feats_eval
                else: # å¦‚æœæ²¡æœ‰MLPï¼Œç›´æ¥ç”¨åŸå§‹numpyæ•°ç»„
                    drug_fg_eval, drug_features_eval, drug_bert_eval, \
                    microbe_features_eval, microbe_bert_eval, microbe_path_eval = \
                    drug_fg, drug_features, drug_bert, microbe_features, microbe_bert, microbe_path

                # è¯„ä¼° DrugVirus æ€§èƒ½
                # test_auc, test_aupr = evaluate_gcn(
                #     model, decoder, drugvirus_test_data, edge_index, edge_weight,
                #     drug_fg_eval, drug_features_eval, drug_bert_eval,
                #     microbe_features_eval, microbe_bert_eval, microbe_path_eval,
                #     microbe_offset, device
                # )
                # è¯„ä¼° DrugVirus æ€§èƒ½
                test_auc, test_aupr, test_acc = evaluate_gcn(
                    model, decoder, drugvirus_test_data, edge_index, edge_weight,
                    drug_fg_eval, drug_features_eval, drug_bert_eval,
                    microbe_features_eval, microbe_bert_eval, microbe_path_eval,
                    microbe_offset, device
                )

                plot_epochs.append(epoch + 1)
                plot_aucs.append(test_auc)
                plot_auprs.append(test_aupr)

                # è¯„ä¼° MDAD æ€§èƒ½
                # mdad_auc, mdad_aupr = evaluate_gcn(
                #     model, decoder, mdad_test_data, mdad_edge_index, mdad_edge_weight,
                #     mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
                #     mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
                #     mdad_microbe_offset, device
                # )
                # è¯„ä¼° MDAD æ€§èƒ½
                # è¯„ä¼° MDAD æ€§èƒ½
                mdad_auc, mdad_aupr, mdad_acc = evaluate_gcn(
                    model, decoder, mdad_test_data, mdad_edge_index, mdad_edge_weight,
                    mdad_drug_fg_norm, mdad_drug_features_norm, mdad_drug_bert_norm,
                    mdad_microbe_features_norm, mdad_microbe_bert_norm, mdad_microbe_path_norm,
                    mdad_microbe_offset, device
                )

                mdad_plot_epochs.append(epoch + 1)
                mdad_plot_aucs.append(mdad_auc)
                mdad_plot_auprs.append(mdad_aupr)

            model.train()
            decoder.train()
            if alignment_mlps: alignment_mlps.train()

        if (epoch + 1) % 40 == 0:
            print(
                f"[EWC-MTL Fold {fold_num + 1}] Epoch {epoch + 1}/{epochs}, "
                f"Loss_DV: {loss_drugvirus.item():.4f}, "
                f"Loss_MDAD: {loss_mdad.item():.4f}, "
                f"Loss_EWC: {ewc_loss.item():.4f}, "
                f"Total: {total_loss.item():.4f}"
            )

    # --- è®­ç»ƒç»“æŸåç»˜å›¾å¹¶ä¿å­˜ (é€»è¾‘ä¸å˜) ---
    if plot_epochs:
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        plt.figure(figsize=(12, 8))
        plt.plot(plot_epochs, plot_aucs, marker='o', linestyle='-', label='DrugVirus Test AUC')
        plt.plot(plot_epochs, plot_auprs, marker='s', linestyle='--', label='DrugVirus Test AUPR')
        plt.title(f'Incremental Learning Curve (Fold {fold_num + 1})', fontsize=16)
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Score', fontsize=12)
        plt.legend(fontsize=12)
        plt.grid(True)
        os.makedirs(save_dir, exist_ok=True)
        plot_filename = f'incremental_learning_fold_{fold_num + 1}.png'
        save_path = os.path.join(save_dir, plot_filename)
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"æˆåŠŸ: å¢é‡å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜ -> {save_path}")

    if mdad_plot_epochs:
        plt.figure(figsize=(12, 8))
        plt.plot(mdad_plot_epochs, mdad_plot_aucs, marker='o', label=f'{args.dataset} AUC')
        plt.plot(mdad_plot_epochs, mdad_plot_auprs, marker='s', label=f'{args.dataset} AUPR')
        plt.title(f'{args.dataset} Performance vs Epoch (Fold {fold_num + 1})', fontsize=16)
        plt.xlabel('Epoch')
        plt.ylabel('Score')
        plt.legend()
        plt.grid(True)
        os.makedirs(save_dir, exist_ok=True)
        mdad_path = os.path.join(save_dir, f'{args.dataset.lower()}_epoch_curve_fold_{fold_num + 1}.png')
        plt.savefig(mdad_path, dpi=300)
        plt.close()
        print(f"{args.dataset}æ—§ä»»åŠ¡æ›²çº¿å·²ä¿å­˜è‡³: {mdad_path}")

    return model, decoder


# =================================================================================
#  è¯·å°†æ­¤æ–°å‡½æ•°æ·»åŠ åˆ° train_eval.py æ–‡ä»¶ä¸­ï¼ˆå¯ä»¥æ”¾åœ¨æ–‡ä»¶é å‰çš„ä½ç½®ï¼‰
# =================================================================================
def pretrain_alignment_mlp_by_stats(
        source_feats,
        target_feats,
        device,
        epochs=150,
        lr=0.005
):
    """
    ã€æ–°å¢å‡½æ•°ã€‘
    ä¸ºå¢é‡å­¦ä¹ é¢„è®­ç»ƒç‰¹å¾å¯¹é½MLPã€‚
    ç›®æ ‡ï¼šå°† source_feats (DrugVirus) é€šè¿‡MLPå˜æ¢åï¼Œä½¿å…¶è¾“å‡ºçš„å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®
          ä¸ target_feats (MDAD) çš„å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®ç›¸åŒ¹é…ã€‚
    è¿™ä¸ªå‡½æ•°æ˜¯ç‹¬ç«‹çš„ï¼Œåªåœ¨å¢é‡å­¦ä¹ å‰è°ƒç”¨ä¸€æ¬¡ã€‚
    """
    import torch.nn as nn
    import torch.optim as optim

    # 1. åˆ›å»ºMLPåˆ—è¡¨ï¼Œæ¯ä¸ªç‰¹å¾ä¸€ä¸ª
    mlp_list = []
    for src_feat, tgt_feat in zip(source_feats, target_feats):
        in_dim = src_feat.shape[1]
        out_dim = tgt_feat.shape[1]
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„MLPæ¥è¿›è¡Œç»´åº¦å˜æ¢å’Œåˆ†å¸ƒå¯¹é½
        mlp = nn.Sequential(
            nn.Linear(in_dim, (in_dim + out_dim) // 2),
            nn.ReLU(),
            nn.Linear((in_dim + out_dim) // 2, out_dim)
        ).to(device)
        mlp_list.append(mlp)

    # å°†MLPåˆ—è¡¨å°è£…æˆnn.ModuleListï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨èƒ½è¯†åˆ«æ‰€æœ‰å‚æ•°
    alignment_mlps = nn.ModuleList(mlp_list)
    optimizer = optim.Adam(alignment_mlps.parameters(), lr=lr)

    # 2. è®¡ç®—ç›®æ ‡ç‰¹å¾ï¼ˆMDADï¼‰çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆåªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
    target_means = [torch.tensor(t.mean(axis=0), dtype=torch.float32, device=device) for t in target_feats]
    target_stds = [torch.tensor(t.std(axis=0), dtype=torch.float32, device=device) for t in target_feats]

    # å°†æºç‰¹å¾ï¼ˆDrugVirusï¼‰è½¬ä¸ºTensor
    source_tensors = [torch.tensor(s, dtype=torch.float32, device=device) for s in source_feats]

    print("===== å¼€å§‹ä¸ºå¢é‡å­¦ä¹ é¢„è®­ç»ƒç‰¹å¾å¯¹é½MLP (åŸºäºå…¨å±€ç»Ÿè®¡é‡) =====")
    alignment_mlps.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        total_loss = 0

        # 3. å¯¹æ¯ä¸€ç§ç‰¹å¾ï¼Œè®¡ç®—å…¶å˜æ¢åçš„åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒçš„å·®è·
        for i, mlp in enumerate(alignment_mlps):
            # å°†æºç‰¹å¾é€šè¿‡MLPè¿›è¡Œå˜æ¢
            predicted_feat = mlp(source_tensors[i])

            # è®¡ç®—å˜æ¢åç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
            pred_mean = predicted_feat.mean(dim=0)
            pred_std = predicted_feat.std(dim=0)

            # æŸå¤±å‡½æ•° = å‡å€¼MSE + æ ‡å‡†å·®MSE
            loss = nn.functional.mse_loss(pred_mean, target_means[i]) + \
                   nn.functional.mse_loss(pred_std, target_stds[i])
            total_loss += loss

        total_loss.backward()
        optimizer.step()

        if (epoch + 1) % 30 == 0:
            print(f"[å¯¹é½é¢„è®­ç»ƒ] Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.6f}")

    print("===== ç‰¹å¾å¯¹é½MLPé¢„è®­ç»ƒå®Œæˆ =====")
    alignment_mlps.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    return alignment_mlps


# train_eval.py æ–‡ä»¶ä¸­

import torch.nn as nn
import torch.optim as optim


def pretrain_alignment_mlp_by_stats_v2(
        source_feats,
        target_feats,
        device,
        epochs=150,
        lr=0.005
):
    """
    ã€æ”¹è¿›ç‰ˆã€‘
    ä¸ºå¢é‡å­¦ä¹ é¢„è®­ç»ƒç‰¹å¾å¯¹é½MLPã€‚
    ç»“åˆäº†ã€æ­£äº¤åˆå§‹åŒ–ã€‘å’Œã€BatchNorm1dã€‘ï¼Œä½¿è®­ç»ƒæ›´ç¨³å®šé«˜æ•ˆã€‚
    """
    mlp_list = []
    for src_feat, tgt_feat in zip(source_feats, target_feats):
        in_dim = src_feat.shape[1]
        out_dim = tgt_feat.shape[1]

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ”¹è¿›MLPç»“æ„å¹¶åº”ç”¨æ›´å¥½çš„åˆå§‹åŒ– ---
        mlp = nn.Sequential(
            nn.Linear(in_dim, (in_dim + out_dim) // 2),
            nn.BatchNorm1d((in_dim + out_dim) // 2),  # æ·»åŠ BatchNormå±‚
            nn.ReLU(),
            nn.Linear((in_dim + out_dim) // 2, out_dim)
        ).to(device)

        # --- å¯¹çº¿æ€§å±‚åº”ç”¨æ­£äº¤åˆå§‹åŒ– ---
        for layer in mlp:
            if isinstance(layer, nn.Linear):
                nn.init.orthogonal_(layer.weight)  # åº”ç”¨æ­£äº¤åˆå§‹åŒ–
                if layer.bias is not None:
                    nn.init.zeros_(layer.bias)  # åç½®åˆå§‹åŒ–ä¸º0

        mlp_list.append(mlp)

    alignment_mlps = nn.ModuleList(mlp_list)
    optimizer = optim.Adam(alignment_mlps.parameters(), lr=lr)

    target_means = [torch.tensor(t.mean(axis=0), dtype=torch.float32, device=device) for t in target_feats]
    target_stds = [torch.tensor(t.std(axis=0), dtype=torch.float32, device=device) for t in target_feats]
    source_tensors = [torch.tensor(s, dtype=torch.float32, device=device) for s in source_feats]

    print("===== å¼€å§‹ä¸ºå¢é‡å­¦ä¹ é¢„è®­ç»ƒç‰¹å¾å¯¹é½MLP (V2: æ­£äº¤åˆå§‹åŒ– + BatchNorm) =====")
    alignment_mlps.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        total_loss = 0

        for i, mlp in enumerate(alignment_mlps):
            predicted_feat = mlp(source_tensors[i])
            pred_mean = predicted_feat.mean(dim=0)
            pred_std = predicted_feat.std(dim=0)

            loss = nn.functional.mse_loss(pred_mean, target_means[i]) + \
                   nn.functional.mse_loss(pred_std, target_stds[i])
            total_loss += loss

        total_loss.backward()
        optimizer.step()

        if (epoch + 1) % 30 == 0:
            print(f"[å¯¹é½é¢„è®­ç»ƒ V2] Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.6f}")

    print("===== ç‰¹å¾å¯¹é½MLPé¢„è®­ç»ƒå®Œæˆ (V2) =====")
    alignment_mlps.eval()
    return alignment_mlps


import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from tqdm import tqdm


def _ensure_tensor_concat(feat_block, device):
    """
    Accept numpy / tensor / list(tuple) of them, flatten to single tensor on device.
    """
    if isinstance(feat_block, (np.ndarray, torch.Tensor)):
        tensors = [feat_block]
    elif isinstance(feat_block, (list, tuple)):
        tensors = feat_block
    else:
        raise TypeError(f"Unsupported feature container type: {type(feat_block)}")

    torch_list = []
    for item in tensors:
        if isinstance(item, np.ndarray):
            torch_list.append(torch.from_numpy(item))
        elif torch.is_tensor(item):
            torch_list.append(item.detach().cpu())
        else:
            raise TypeError(f"Unsupported feature element type: {type(item)}")
    return torch.cat(torch_list, dim=0).to(device=device, dtype=torch.float32)


def _torch_cov(x):
    x = x - x.mean(dim=0, keepdim=True)
    return x.t() @ x / (x.size(0) - 1)


def _rbf_kernel(x, y, gamma):
    x_norm = (x ** 2).sum(dim=1).view(-1, 1)
    y_norm = (y ** 2).sum(dim=1).view(1, -1)
    dist = x_norm + y_norm - 2 * x @ y.t()
    return torch.exp(-gamma * dist.clamp_min_(0.0))


import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from tqdm import tqdm


def _ensure_tensor_concat(block, device):
    if isinstance(block, (np.ndarray, torch.Tensor)):
        items = [block]
    elif isinstance(block, (list, tuple)):
        items = block
    else:
        raise TypeError(f"Unsupported type: {type(block)}")

    tensors = []
    for item in items:
        if isinstance(item, np.ndarray):
            tensors.append(torch.from_numpy(item))
        elif torch.is_tensor(item):
            tensors.append(item.detach().cpu())
        else:
            raise TypeError(f"Unsupported element type: {type(item)}")
    return torch.cat(tensors, dim=0).to(device=device, dtype=torch.float32)


def _torch_cov(x):
    x = x - x.mean(dim=0, keepdim=True)
    return x.t() @ x / (x.size(0) - 1)


def _rbf_kernel(x, y, gamma):
    x_norm = (x ** 2).sum(dim=1).view(-1, 1)
    y_norm = (y ** 2).sum(dim=1).view(1, -1)
    dist = x_norm + y_norm - 2 * x @ y.t()
    return torch.exp(-gamma * dist.clamp_min_(0.0))


# def pretrain_alignment_mlp_by_stats_v3(
#         source_feats,
#         target_feats,
#         device,
#         epochs: int = 400,
#         lr: float = 5e-4,
#         hidden_ratio: float = 0.5,
#         proj_hidden_ratio: float = 0.75,
#         mmd_gamma: float = 0.01,
#         rand_pair_weight: float = 0.1,
#         mmd_weight: float = 0.3,
#         cov_weight: float = 0.5,
#         mean_weight: float = 0.5,
#         std_weight: float = 0.5,
#         verbose: bool = True,
# ):
#     """
#     ç»´åº¦å¯ä¸åŒï¼›å¹³å‡å€¼/æ–¹å·®/åæ–¹å·®/MMD/éšæœºé…å¯¹å¤šé‡çº¦æŸã€‚
#     è¾“å…¥å¯ä¸º numpyã€tensor æˆ– list/tupleã€‚
#     è¿”å› nn.ModuleListã€‚
#     """
#     if len(source_feats) != len(target_feats):
#         raise ValueError("source_feats ä¸ target_feats çš„é•¿åº¦å¿…é¡»ä¸€è‡´ã€‚")
#
#     mlp_list = nn.ModuleList().to(device)
#     optimizer_params = []
#
#     for idx, (src_block, tgt_block) in enumerate(zip(source_feats, target_feats)):
#         src_all = _ensure_tensor_concat(src_block, device)
#         tgt_all = _ensure_tensor_concat(tgt_block, device)
#
#         in_dim = src_all.size(1)
#         out_dim = tgt_all.size(1)
#         hidden_dim = max(8, int(in_dim * hidden_ratio))
#         proj_hidden = max(8, int((in_dim + out_dim) * proj_hidden_ratio * 0.5))
#
#         mlp = nn.Sequential(
#             nn.Linear(in_dim, hidden_dim),
#             nn.ReLU(inplace=True),
#             nn.Linear(hidden_dim, proj_hidden),
#             nn.ReLU(inplace=True),
#             nn.Linear(proj_hidden, out_dim)
#         ).to(device)
#
#         nn.init.orthogonal_(mlp[0].weight); nn.init.zeros_(mlp[0].bias)
#         nn.init.orthogonal_(mlp[2].weight); nn.init.zeros_(mlp[2].bias)
#         nn.init.orthogonal_(mlp[4].weight); nn.init.zeros_(mlp[4].bias)
#
#         mlp_list.append(mlp)
#         optimizer_params += list(mlp.parameters())
#
#     optimizer = Adam(optimizer_params, lr=lr, weight_decay=1e-5)
#     loss_log = []
#
#     iterator = tqdm(range(epochs), desc="Align v4") if verbose else range(epochs)
#
#     with torch.no_grad():
#         target_stats = []
#         for tgt_block in target_feats:
#             tgt_all = _ensure_tensor_concat(tgt_block, device)
#             target_stats.append({
#                 "mean": tgt_all.mean(dim=0),
#                 "std": tgt_all.std(dim=0),
#                 "cov": _torch_cov(tgt_all),
#                 "samples": tgt_all
#             })
#
#     for epoch in iterator:
#         optimizer.zero_grad()
#         total_loss = 0.0
#
#         for feat_idx, (mlp, tgt_stat) in enumerate(zip(mlp_list, target_stats)):
#             src_all = _ensure_tensor_concat(source_feats[feat_idx], device)
#             transformed = mlp(src_all)
#
#             mean_loss = F.mse_loss(transformed.mean(dim=0), tgt_stat["mean"])
#             std_loss = F.mse_loss(transformed.std(dim=0), tgt_stat["std"])
#             cov_loss = F.mse_loss(_torch_cov(transformed), tgt_stat["cov"])
#
#             k_xx = _rbf_kernel(transformed, transformed, mmd_gamma).mean()
#             k_yy = _rbf_kernel(tgt_stat["samples"], tgt_stat["samples"], mmd_gamma).mean()
#             k_xy = _rbf_kernel(transformed, tgt_stat["samples"], mmd_gamma).mean()
#             mmd_loss = k_xx + k_yy - 2 * k_xy
#
#             rand_idx = torch.randint(0, tgt_stat["samples"].size(0), (transformed.size(0),), device=device)
#             pair_loss = F.l1_loss(transformed, tgt_stat["samples"][rand_idx])
#
#             loss = (mean_weight * mean_loss +
#                     std_weight * std_loss +
#                     cov_weight * cov_loss +
#                     mmd_weight * mmd_loss +
#                     rand_pair_weight * pair_loss)
#             total_loss += loss
#
#         total_loss.backward()
#         optimizer.step()
#
#         if verbose:
#             loss_log.append(total_loss.item())
#             iterator.set_postfix(loss=f"{total_loss.item():.4f}")
#
#     if verbose and loss_log:
#         print(f"[Align v4] Final loss: {loss_log[-1]:.6f}")
#
#     return mlp_list


# In your utils.py or equivalent file

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm


# Helper function to compute covariance matrix
def torch_cov(tensor, t=None):
    """
    Computes the covariance matrix of a 2D tensor.
    tensor: (n_samples, n_features)
    """
    if t is None:
        t = tensor
    # Subtract the mean
    tensor_mean = torch.mean(tensor, dim=0, keepdim=True)
    t_mean = torch.mean(t, dim=0, keepdim=True)

    # (n_features, n_samples) x (n_samples, n_features) -> (n_features, n_features)
    cov = (tensor - tensor_mean).t() @ (t - t_mean) / (tensor.size(0) - 1)
    return cov

